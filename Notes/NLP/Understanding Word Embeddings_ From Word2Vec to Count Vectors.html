<!DOCTYPE html>
<!-- saved from url=(0077)https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/ -->
<html class="js csstransforms3d wf-roboto-n4-active wf-raleway-n6-active wf-opensans-n7-active wf-active"><link type="text/css" rel="stylesheet" id="dark-mode-general-link"><link type="text/css" rel="stylesheet" id="dark-mode-custom-link"><style type="text/css" id="dark-mode-custom-style"></style><head lang="en-US"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<meta name="google-play-app" content="app-id=com.analyticsvidhya.android">
<meta name="viewport" content="width=device-width, initial-scale=1.0">


<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<link href="https://www.google-analytics.com/" rel="dns-prefetch">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<link rel="pingback" href="https://www.analyticsvidhya.com/xmlrpc.php">
<link rel="icon" id="favicon" type="image/png" href="https://www.analyticsvidhya.com/wp-content/uploads/2015/02/logo_square_v2.jpg"><link rel="apple-touch-icon" href="https://www.analyticsvidhya.com/wp-content/uploads/2015/02/logo_square_v2.jpg"><link rel="apple-touch-icon" sizes="76x76" href="https://www.analyticsvidhya.com/wp-content/uploads/2015/02/logo_square_v2.jpg"><link rel="apple-touch-icon" sizes="114x114" href="https://www.analyticsvidhya.com/wp-content/uploads/2015/02/logo_square_v2.jpg"><link rel="apple-touch-icon" sizes="144x144" href="https://www.analyticsvidhya.com/wp-content/uploads/2015/02/logo_square_v2.jpg"><link href="https://www.analyticsvidhya.com/wp-content/themes/Curated/custom/custom_style.css?ver=4.9.7.17" rel="preload stylesheet" as="style">
<link href="https://www.analyticsvidhya.com/wp-content/themes/Curated/custom/footer_style.css?ver=1.1.1" rel="preload stylesheet" as="style">
<link href="https://www.analyticsvidhya.com/wp-content/themes/Curated/custom/special-offer-strip-2.css?ver=2.3.1" rel="preload stylesheet" as="style">
<link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="preload stylesheet" as="style">
<link href="https://www.analyticsvidhya.com/wp-content/themes/Curated/static/css/bootstrap-responsive.css?ver=1.0" rel="preload stylesheet" as="style">
<link href="https://www.analyticsvidhya.com/wp-content/themes/Curated/static/css/font-tm.css?ver=1.0" rel="preload stylesheet" as="style">
<link href="https://www.analyticsvidhya.com/wp-content/themes/Curated/static/css/basix.css?ver=1.0" rel="preload stylesheet" as="style">
<link href="https://www.analyticsvidhya.com/wp-content/themes/Curated/static/css/basix-responsive.css?ver=1.0" rel="preload stylesheet" as="style">
<div class="fit-vids-style" id="fit-vids-style" style="display: none;">Â­<style>.fluid-width-video-wrapper{width:100%;position:relative;padding:0;}.fluid-width-video-wrapper iframe,.fluid-width-video-wrapper object,.fluid-width-video-wrapper embed {position:absolute;top:0;left:0;width:100%;height:100%;}</style></div><script async="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/analytics.js"></script><script async="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/gtm.js"></script><script src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/webfont.js" type="text/javascript" async=""></script><script>
                            /* You can add more configuration options to webfontloader by previously defining the WebFontConfig with your options */
                            if ( typeof WebFontConfig === "undefined" ) {
                                WebFontConfig = new Object();
                            }
                            WebFontConfig['google'] = {families: ['Open+Sans:700', 'Raleway:600', 'Roboto:400&amp;subset=latin']};

                            (function() {
                                var wf = document.createElement( 'script' );
                                wf.src = 'https://ajax.googleapis.com/ajax/libs/webfont/1.5.3/webfont.js';
                                wf.type = 'text/javascript';
                                wf.async = 'true';
                                var s = document.getElementsByTagName( 'script' )[0];
                                s.parentNode.insertBefore( wf, s );
                            })();
                        </script>
<style>
.stb-container-css {margin: 10px 10px 10px 10px;}.stb-box {}.stb-caption-box {}.stb-body-box {}
/* Class Dependent Parameters */
.stb-border.stb-alert-container {border: 1px none #FF4F4A;}.stb-side.stb-alert-container {background: #FF4F4A;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#FF4F4A', endColorstr='#504848',GradientType=0 );background: -moz-linear-gradient(top,  #FF4F4A 30%, #504848 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#FF4F4A), color-stop(90%,#504848));background: -webkit-linear-gradient(top,  #FF4F4A 30%,#504848 90%);background: -o-linear-gradient(top,  #FF4F4A 30%,#504848 90%);background: -ms-linear-gradient(top,  #FF4F4A 30%,#504848 90%);background: linear-gradient(#FF4F4A 30%, #504848 90%);}.stb-side-none.stb-alert-container {background: #FFE7E6;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#FFE7E6', endColorstr='#fb7d78',GradientType=0 );background: -moz-linear-gradient(top,  #FFE7E6 30%, #fb7d78 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#FFE7E6), color-stop(90%,#fb7d78));background: -webkit-linear-gradient(top,  #FFE7E6 30%,#fb7d78 90%);background: -o-linear-gradient(top,  #FFE7E6 30%,#fb7d78 90%);background: -ms-linear-gradient(top,  #FFE7E6 30%,#fb7d78 90%);background: linear-gradient(#FFE7E6 30%, #fb7d78 90%);}.stb-alert_box {background: #FFE7E6;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#FFE7E6', endColorstr='#fb7d78',GradientType=0 );background: -moz-linear-gradient(top,  #FFE7E6 30%, #fb7d78 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#FFE7E6), color-stop(90%,#fb7d78));background: -webkit-linear-gradient(top,  #FFE7E6 30%,#fb7d78 90%);background: -o-linear-gradient(top,  #FFE7E6 30%,#fb7d78 90%);background: -ms-linear-gradient(top,  #FFE7E6 30%,#fb7d78 90%);background: linear-gradient(#FFE7E6 30%, #fb7d78 90%);color: #000000;}.stb-alert-caption_box {background: #FF4F4A;background: -moz-linear-gradient(top,  #FF4F4A 30%, #504848 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#FF4F4A), color-stop(90%,#504848));background: -webkit-linear-gradient(top,  #FF4F4A 30%,#504848 90%);background: -o-linear-gradient(top,  #FF4F4A 30%,#504848 90%);background: -ms-linear-gradient(top,  #FF4F4A 30%,#504848 90%);background: linear-gradient(#FF4F4A 30%, #504848 90%);color: #FFFFFF;}.stb-alert-body_box {background: #FFE7E6;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#FFE7E6', endColorstr='#fb7d78',GradientType=0 );background: -moz-linear-gradient(top,  #FFE7E6 30%, #fb7d78 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#FFE7E6), color-stop(90%,#fb7d78));background: -webkit-linear-gradient(top,  #FFE7E6 30%,#fb7d78 90%);background: -o-linear-gradient(top,  #FFE7E6 30%,#fb7d78 90%);background: -ms-linear-gradient(top,  #FFE7E6 30%,#fb7d78 90%);background: linear-gradient(#FFE7E6 30%, #fb7d78 90%);color: #000000;}.stb-border.stb-black-container {border: 1px none #6E6E6E;}.stb-side.stb-black-container {background: #333333;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#333333', endColorstr='#504848',GradientType=0 );background: -moz-linear-gradient(top,  #333333 30%, #504848 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#333333), color-stop(90%,#504848));background: -webkit-linear-gradient(top,  #333333 30%,#504848 90%);background: -o-linear-gradient(top,  #333333 30%,#504848 90%);background: -ms-linear-gradient(top,  #333333 30%,#504848 90%);background: linear-gradient(#333333 30%, #504848 90%);}.stb-side-none.stb-black-container {background: #000000;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#000000', endColorstr='#000000',GradientType=0 );background: -moz-linear-gradient(top,  #000000 30%, #000000 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#000000), color-stop(90%,#000000));background: -webkit-linear-gradient(top,  #000000 30%,#000000 90%);background: -o-linear-gradient(top,  #000000 30%,#000000 90%);background: -ms-linear-gradient(top,  #000000 30%,#000000 90%);background: linear-gradient(#000000 30%, #000000 90%);}.stb-black_box {background: #000000;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#000000', endColorstr='#000000',GradientType=0 );background: -moz-linear-gradient(top,  #000000 30%, #000000 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#000000), color-stop(90%,#000000));background: -webkit-linear-gradient(top,  #000000 30%,#000000 90%);background: -o-linear-gradient(top,  #000000 30%,#000000 90%);background: -ms-linear-gradient(top,  #000000 30%,#000000 90%);background: linear-gradient(#000000 30%, #000000 90%);color: #FFFFFF;}.stb-black-caption_box {background: #333333;background: -moz-linear-gradient(top,  #333333 30%, #504848 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#333333), color-stop(90%,#504848));background: -webkit-linear-gradient(top,  #333333 30%,#504848 90%);background: -o-linear-gradient(top,  #333333 30%,#504848 90%);background: -ms-linear-gradient(top,  #333333 30%,#504848 90%);background: linear-gradient(#333333 30%, #504848 90%);color: #FFFFFF;}.stb-black-body_box {background: #000000;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#000000', endColorstr='#000000',GradientType=0 );background: -moz-linear-gradient(top,  #000000 30%, #000000 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#000000), color-stop(90%,#000000));background: -webkit-linear-gradient(top,  #000000 30%,#000000 90%);background: -o-linear-gradient(top,  #000000 30%,#000000 90%);background: -ms-linear-gradient(top,  #000000 30%,#000000 90%);background: linear-gradient(#000000 30%, #000000 90%);color: #FFFFFF;}.stb-border.stb-custom-container {border: 1px none #f844ee;}.stb-side.stb-custom-container {background: #f844ee;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#f844ee', endColorstr='#504848',GradientType=0 );background: -moz-linear-gradient(top,  #f844ee 30%, #504848 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#f844ee), color-stop(90%,#504848));background: -webkit-linear-gradient(top,  #f844ee 30%,#504848 90%);background: -o-linear-gradient(top,  #f844ee 30%,#504848 90%);background: -ms-linear-gradient(top,  #f844ee 30%,#504848 90%);background: linear-gradient(#f844ee 30%, #504848 90%);}.stb-side-none.stb-custom-container {background: #f7cdf5;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#f7cdf5', endColorstr='#f77df1',GradientType=0 );background: -moz-linear-gradient(top,  #f7cdf5 30%, #f77df1 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#f7cdf5), color-stop(90%,#f77df1));background: -webkit-linear-gradient(top,  #f7cdf5 30%,#f77df1 90%);background: -o-linear-gradient(top,  #f7cdf5 30%,#f77df1 90%);background: -ms-linear-gradient(top,  #f7cdf5 30%,#f77df1 90%);background: linear-gradient(#f7cdf5 30%, #f77df1 90%);}.stb-custom_box {background: #f7cdf5;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#f7cdf5', endColorstr='#f77df1',GradientType=0 );background: -moz-linear-gradient(top,  #f7cdf5 30%, #f77df1 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#f7cdf5), color-stop(90%,#f77df1));background: -webkit-linear-gradient(top,  #f7cdf5 30%,#f77df1 90%);background: -o-linear-gradient(top,  #f7cdf5 30%,#f77df1 90%);background: -ms-linear-gradient(top,  #f7cdf5 30%,#f77df1 90%);background: linear-gradient(#f7cdf5 30%, #f77df1 90%);color: #000000;}.stb-custom-caption_box {background: #f844ee;background: -moz-linear-gradient(top,  #f844ee 30%, #504848 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#f844ee), color-stop(90%,#504848));background: -webkit-linear-gradient(top,  #f844ee 30%,#504848 90%);background: -o-linear-gradient(top,  #f844ee 30%,#504848 90%);background: -ms-linear-gradient(top,  #f844ee 30%,#504848 90%);background: linear-gradient(#f844ee 30%, #504848 90%);color: #ffffff;}.stb-custom-body_box {background: #f7cdf5;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#f7cdf5', endColorstr='#f77df1',GradientType=0 );background: -moz-linear-gradient(top,  #f7cdf5 30%, #f77df1 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#f7cdf5), color-stop(90%,#f77df1));background: -webkit-linear-gradient(top,  #f7cdf5 30%,#f77df1 90%);background: -o-linear-gradient(top,  #f7cdf5 30%,#f77df1 90%);background: -ms-linear-gradient(top,  #f7cdf5 30%,#f77df1 90%);background: linear-gradient(#f7cdf5 30%, #f77df1 90%);color: #000000;}.stb-border.stb-download-container {border: 1px none #65ADFE;}.stb-side.stb-download-container {background: #65ADFE;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#65ADFE', endColorstr='#504848',GradientType=0 );background: -moz-linear-gradient(top,  #65ADFE 30%, #504848 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#65ADFE), color-stop(90%,#504848));background: -webkit-linear-gradient(top,  #65ADFE 30%,#504848 90%);background: -o-linear-gradient(top,  #65ADFE 30%,#504848 90%);background: -ms-linear-gradient(top,  #65ADFE 30%,#504848 90%);background: linear-gradient(#65ADFE 30%, #504848 90%);}.stb-side-none.stb-download-container {background: #DFF0FF;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#DFF0FF', endColorstr='#2e7cb9',GradientType=0 );background: -moz-linear-gradient(top,  #DFF0FF 30%, #2e7cb9 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#DFF0FF), color-stop(90%,#2e7cb9));background: -webkit-linear-gradient(top,  #DFF0FF 30%,#2e7cb9 90%);background: -o-linear-gradient(top,  #DFF0FF 30%,#2e7cb9 90%);background: -ms-linear-gradient(top,  #DFF0FF 30%,#2e7cb9 90%);background: linear-gradient(#DFF0FF 30%, #2e7cb9 90%);}.stb-download_box {background: #DFF0FF;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#DFF0FF', endColorstr='#2e7cb9',GradientType=0 );background: -moz-linear-gradient(top,  #DFF0FF 30%, #2e7cb9 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#DFF0FF), color-stop(90%,#2e7cb9));background: -webkit-linear-gradient(top,  #DFF0FF 30%,#2e7cb9 90%);background: -o-linear-gradient(top,  #DFF0FF 30%,#2e7cb9 90%);background: -ms-linear-gradient(top,  #DFF0FF 30%,#2e7cb9 90%);background: linear-gradient(#DFF0FF 30%, #2e7cb9 90%);color: #000000;}.stb-download-caption_box {background: #65ADFE;background: -moz-linear-gradient(top,  #65ADFE 30%, #504848 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#65ADFE), color-stop(90%,#504848));background: -webkit-linear-gradient(top,  #65ADFE 30%,#504848 90%);background: -o-linear-gradient(top,  #65ADFE 30%,#504848 90%);background: -ms-linear-gradient(top,  #65ADFE 30%,#504848 90%);background: linear-gradient(#65ADFE 30%, #504848 90%);color: #FFFFFF;}.stb-download-body_box {background: #DFF0FF;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#DFF0FF', endColorstr='#2e7cb9',GradientType=0 );background: -moz-linear-gradient(top,  #DFF0FF 30%, #2e7cb9 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#DFF0FF), color-stop(90%,#2e7cb9));background: -webkit-linear-gradient(top,  #DFF0FF 30%,#2e7cb9 90%);background: -o-linear-gradient(top,  #DFF0FF 30%,#2e7cb9 90%);background: -ms-linear-gradient(top,  #DFF0FF 30%,#2e7cb9 90%);background: linear-gradient(#DFF0FF 30%, #2e7cb9 90%);color: #000000;}.stb-border.stb-grey-container {border: 1px none #BBBBBB;}.stb-side.stb-grey-container {background: #BBBBBB;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#BBBBBB', endColorstr='#6e6e6e',GradientType=0 );background: -moz-linear-gradient(top,  #BBBBBB 30%, #6e6e6e 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#BBBBBB), color-stop(90%,#6e6e6e));background: -webkit-linear-gradient(top,  #BBBBBB 30%,#6e6e6e 90%);background: -o-linear-gradient(top,  #BBBBBB 30%,#6e6e6e 90%);background: -ms-linear-gradient(top,  #BBBBBB 30%,#6e6e6e 90%);background: linear-gradient(#BBBBBB 30%, #6e6e6e 90%);}.stb-side-none.stb-grey-container {background: #EEEEEE;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#EEEEEE', endColorstr='#ababab',GradientType=0 );background: -moz-linear-gradient(top,  #EEEEEE 30%, #ababab 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#EEEEEE), color-stop(90%,#ababab));background: -webkit-linear-gradient(top,  #EEEEEE 30%,#ababab 90%);background: -o-linear-gradient(top,  #EEEEEE 30%,#ababab 90%);background: -ms-linear-gradient(top,  #EEEEEE 30%,#ababab 90%);background: linear-gradient(#EEEEEE 30%, #ababab 90%);}.stb-grey_box {background: #EEEEEE;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#EEEEEE', endColorstr='#ababab',GradientType=0 );background: -moz-linear-gradient(top,  #EEEEEE 30%, #ababab 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#EEEEEE), color-stop(90%,#ababab));background: -webkit-linear-gradient(top,  #EEEEEE 30%,#ababab 90%);background: -o-linear-gradient(top,  #EEEEEE 30%,#ababab 90%);background: -ms-linear-gradient(top,  #EEEEEE 30%,#ababab 90%);background: linear-gradient(#EEEEEE 30%, #ababab 90%);color: #000000;}.stb-grey-caption_box {background: #BBBBBB;background: -moz-linear-gradient(top,  #BBBBBB 30%, #6e6e6e 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#BBBBBB), color-stop(90%,#6e6e6e));background: -webkit-linear-gradient(top,  #BBBBBB 30%,#6e6e6e 90%);background: -o-linear-gradient(top,  #BBBBBB 30%,#6e6e6e 90%);background: -ms-linear-gradient(top,  #BBBBBB 30%,#6e6e6e 90%);background: linear-gradient(#BBBBBB 30%, #6e6e6e 90%);color: #FFFFFF;}.stb-grey-body_box {background: #EEEEEE;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#EEEEEE', endColorstr='#ababab',GradientType=0 );background: -moz-linear-gradient(top,  #EEEEEE 30%, #ababab 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#EEEEEE), color-stop(90%,#ababab));background: -webkit-linear-gradient(top,  #EEEEEE 30%,#ababab 90%);background: -o-linear-gradient(top,  #EEEEEE 30%,#ababab 90%);background: -ms-linear-gradient(top,  #EEEEEE 30%,#ababab 90%);background: linear-gradient(#EEEEEE 30%, #ababab 90%);color: #000000;}.stb-border.stb-info-container {border: 1px none #7AD975;}.stb-side.stb-info-container {background: #7AD975;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#7AD975', endColorstr='#504848',GradientType=0 );background: -moz-linear-gradient(top,  #7AD975 30%, #504848 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#7AD975), color-stop(90%,#504848));background: -webkit-linear-gradient(top,  #7AD975 30%,#504848 90%);background: -o-linear-gradient(top,  #7AD975 30%,#504848 90%);background: -ms-linear-gradient(top,  #7AD975 30%,#504848 90%);background: linear-gradient(#7AD975 30%, #504848 90%);}.stb-side-none.stb-info-container {background: #E2F8DE;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#E2F8DE', endColorstr='#79b06e',GradientType=0 );background: -moz-linear-gradient(top,  #E2F8DE 30%, #79b06e 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#E2F8DE), color-stop(90%,#79b06e));background: -webkit-linear-gradient(top,  #E2F8DE 30%,#79b06e 90%);background: -o-linear-gradient(top,  #E2F8DE 30%,#79b06e 90%);background: -ms-linear-gradient(top,  #E2F8DE 30%,#79b06e 90%);background: linear-gradient(#E2F8DE 30%, #79b06e 90%);}.stb-info_box {background: #E2F8DE;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#E2F8DE', endColorstr='#79b06e',GradientType=0 );background: -moz-linear-gradient(top,  #E2F8DE 30%, #79b06e 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#E2F8DE), color-stop(90%,#79b06e));background: -webkit-linear-gradient(top,  #E2F8DE 30%,#79b06e 90%);background: -o-linear-gradient(top,  #E2F8DE 30%,#79b06e 90%);background: -ms-linear-gradient(top,  #E2F8DE 30%,#79b06e 90%);background: linear-gradient(#E2F8DE 30%, #79b06e 90%);color: #000000;}.stb-info-caption_box {background: #7AD975;background: -moz-linear-gradient(top,  #7AD975 30%, #504848 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#7AD975), color-stop(90%,#504848));background: -webkit-linear-gradient(top,  #7AD975 30%,#504848 90%);background: -o-linear-gradient(top,  #7AD975 30%,#504848 90%);background: -ms-linear-gradient(top,  #7AD975 30%,#504848 90%);background: linear-gradient(#7AD975 30%, #504848 90%);color: #FFFFFF;}.stb-info-body_box {background: #E2F8DE;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#E2F8DE', endColorstr='#79b06e',GradientType=0 );background: -moz-linear-gradient(top,  #E2F8DE 30%, #79b06e 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#E2F8DE), color-stop(90%,#79b06e));background: -webkit-linear-gradient(top,  #E2F8DE 30%,#79b06e 90%);background: -o-linear-gradient(top,  #E2F8DE 30%,#79b06e 90%);background: -ms-linear-gradient(top,  #E2F8DE 30%,#79b06e 90%);background: linear-gradient(#E2F8DE 30%, #79b06e 90%);color: #000000;}.stb-border.stb-section-container {border: 1px none #2982C5;}.stb-side.stb-section-container {background: #2982C5;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#2982C5', endColorstr='#2944f2',GradientType=0 );background: -moz-linear-gradient(top,  #2982C5 30%, #2944f2 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#2982C5), color-stop(90%,#2944f2));background: -webkit-linear-gradient(top,  #2982C5 30%,#2944f2 90%);background: -o-linear-gradient(top,  #2982C5 30%,#2944f2 90%);background: -ms-linear-gradient(top,  #2982C5 30%,#2944f2 90%);background: linear-gradient(#2982C5 30%, #2944f2 90%);}.stb-side-none.stb-section-container {background: #2982C5;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#2982C5', endColorstr='#294ee3',GradientType=0 );background: -moz-linear-gradient(top,  #2982C5 30%, #294ee3 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#2982C5), color-stop(90%,#294ee3));background: -webkit-linear-gradient(top,  #2982C5 30%,#294ee3 90%);background: -o-linear-gradient(top,  #2982C5 30%,#294ee3 90%);background: -ms-linear-gradient(top,  #2982C5 30%,#294ee3 90%);background: linear-gradient(#2982C5 30%, #294ee3 90%);}.stb-section_box {background: #2982C5;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#2982C5', endColorstr='#294ee3',GradientType=0 );background: -moz-linear-gradient(top,  #2982C5 30%, #294ee3 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#2982C5), color-stop(90%,#294ee3));background: -webkit-linear-gradient(top,  #2982C5 30%,#294ee3 90%);background: -o-linear-gradient(top,  #2982C5 30%,#294ee3 90%);background: -ms-linear-gradient(top,  #2982C5 30%,#294ee3 90%);background: linear-gradient(#2982C5 30%, #294ee3 90%);color: #000000;}.stb-section-caption_box {background: #2982C5;background: -moz-linear-gradient(top,  #2982C5 30%, #2944f2 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#2982C5), color-stop(90%,#2944f2));background: -webkit-linear-gradient(top,  #2982C5 30%,#2944f2 90%);background: -o-linear-gradient(top,  #2982C5 30%,#2944f2 90%);background: -ms-linear-gradient(top,  #2982C5 30%,#2944f2 90%);background: linear-gradient(#2982C5 30%, #2944f2 90%);color: #FFFFFF;}.stb-section-body_box {background: #2982C5;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#2982C5', endColorstr='#294ee3',GradientType=0 );background: -moz-linear-gradient(top,  #2982C5 30%, #294ee3 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#2982C5), color-stop(90%,#294ee3));background: -webkit-linear-gradient(top,  #2982C5 30%,#294ee3 90%);background: -o-linear-gradient(top,  #2982C5 30%,#294ee3 90%);background: -ms-linear-gradient(top,  #2982C5 30%,#294ee3 90%);background: linear-gradient(#2982C5 30%, #294ee3 90%);color: #000000;}.stb-border.stb-warning-container {border: 1px none #FE9A05;}.stb-side.stb-warning-container {background: #FE9A05;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#FE9A05', endColorstr='#504848',GradientType=0 );background: -moz-linear-gradient(top,  #FE9A05 30%, #504848 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#FE9A05), color-stop(90%,#504848));background: -webkit-linear-gradient(top,  #FE9A05 30%,#504848 90%);background: -o-linear-gradient(top,  #FE9A05 30%,#504848 90%);background: -ms-linear-gradient(top,  #FE9A05 30%,#504848 90%);background: linear-gradient(#FE9A05 30%, #504848 90%);}.stb-side-none.stb-warning-container {background: #FEFFD5;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#FEFFD5', endColorstr='#f0d208',GradientType=0 );background: -moz-linear-gradient(top,  #FEFFD5 30%, #f0d208 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#FEFFD5), color-stop(90%,#f0d208));background: -webkit-linear-gradient(top,  #FEFFD5 30%,#f0d208 90%);background: -o-linear-gradient(top,  #FEFFD5 30%,#f0d208 90%);background: -ms-linear-gradient(top,  #FEFFD5 30%,#f0d208 90%);background: linear-gradient(#FEFFD5 30%, #f0d208 90%);}.stb-warning_box {background: #FEFFD5;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#FEFFD5', endColorstr='#f0d208',GradientType=0 );background: -moz-linear-gradient(top,  #FEFFD5 30%, #f0d208 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#FEFFD5), color-stop(90%,#f0d208));background: -webkit-linear-gradient(top,  #FEFFD5 30%,#f0d208 90%);background: -o-linear-gradient(top,  #FEFFD5 30%,#f0d208 90%);background: -ms-linear-gradient(top,  #FEFFD5 30%,#f0d208 90%);background: linear-gradient(#FEFFD5 30%, #f0d208 90%);color: #000000;}.stb-warning-caption_box {background: #FE9A05;background: -moz-linear-gradient(top,  #FE9A05 30%, #504848 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#FE9A05), color-stop(90%,#504848));background: -webkit-linear-gradient(top,  #FE9A05 30%,#504848 90%);background: -o-linear-gradient(top,  #FE9A05 30%,#504848 90%);background: -ms-linear-gradient(top,  #FE9A05 30%,#504848 90%);background: linear-gradient(#FE9A05 30%, #504848 90%);color: #FFFFFF;}.stb-warning-body_box {background: #FEFFD5;filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#FEFFD5', endColorstr='#f0d208',GradientType=0 );background: -moz-linear-gradient(top,  #FEFFD5 30%, #f0d208 90%);background: -webkit-gradient(linear, left top, left bottom, color-stop(30%,#FEFFD5), color-stop(90%,#f0d208));background: -webkit-linear-gradient(top,  #FEFFD5 30%,#f0d208 90%);background: -o-linear-gradient(top,  #FEFFD5 30%,#f0d208 90%);background: -ms-linear-gradient(top,  #FEFFD5 30%,#f0d208 90%);background: linear-gradient(#FEFFD5 30%, #f0d208 90%);color: #000000;}</style>

<title>Understanding Word Embeddings: From Word2Vec to Count Vectors</title>
<meta name="description" content="Word embeddings are techniques used in natural language processing. This includes tools &amp; techiniques like word2vec, TD-IDF, count vectors, etc.">
<meta name="robots" content="index, follow">
<meta name="googlebot" content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1">
<meta name="bingbot" content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1">
<link rel="canonical" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/">
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Understanding Word Embeddings: From Word2Vec to Count Vectors">
<meta property="og:description" content="Word embeddings are techniques used in natural language processing. This includes tools &amp; techiniques like word2vec, TD-IDF, count vectors, etc.">
<meta property="og:url" content="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/">
<meta property="og:site_name" content="Analytics Vidhya">
<meta property="article:publisher" content="https://www.facebook.com/AnalyticsVidhya/">
<meta property="article:published_time" content="2017-06-04T17:45:40+00:00">
<meta property="article:modified_time" content="2020-01-20T10:19:00+00:00">
<meta property="og:image" content="https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/06062705/Word-Vectors.png">
<meta property="og:image:width" content="1505">
<meta property="og:image:height" content="527">
<meta name="twitter:card" content="summary">
<meta name="twitter:creator" content="@analyticsvidhya">
<meta name="twitter:site" content="@analyticsvidhya">
<script type="application/ld+json" class="yoast-schema-graph">{"@context":"https://schema.org","@graph":[{"@type":"WebSite","@id":"https://www.analyticsvidhya.com/#website","url":"https://www.analyticsvidhya.com/","name":"Analytics Vidhya","description":"Learn everything about Analytics","potentialAction":[{"@type":"SearchAction","target":"https://www.analyticsvidhya.com/?s={search_term_string}","query-input":"required name=search_term_string"}],"inLanguage":"en-US"},{"@type":"ImageObject","@id":"https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#primaryimage","inLanguage":"en-US","url":"https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/06062705/Word-Vectors.png","width":1505,"height":527},{"@type":"WebPage","@id":"https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#webpage","url":"https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/","name":"Understanding Word Embeddings: From Word2Vec to Count Vectors","isPartOf":{"@id":"https://www.analyticsvidhya.com/#website"},"primaryImageOfPage":{"@id":"https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#primaryimage"},"datePublished":"2017-06-04T17:45:40+00:00","dateModified":"2020-01-20T10:19:00+00:00","author":{"@id":"https://www.analyticsvidhya.com/#/schema/person/cf0a25e052dcfc8f60458228f18e5da4"},"description":"Word embeddings are techniques used in natural language processing. This includes tools & techiniques like word2vec, TD-IDF, count vectors, etc.","breadcrumb":{"@id":"https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#breadcrumb"},"inLanguage":"en-US","potentialAction":[{"@type":"ReadAction","target":["https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/"]}]},{"@type":"BreadcrumbList","@id":"https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#breadcrumb","itemListElement":[{"@type":"ListItem","position":1,"item":{"@type":"WebPage","@id":"https://www.analyticsvidhya.com/","url":"https://www.analyticsvidhya.com/","name":"Home"}},{"@type":"ListItem","position":2,"item":{"@type":"WebPage","@id":"https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/","url":"https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/","name":"An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec"}}]},{"@type":["Person"],"@id":"https://www.analyticsvidhya.com/#/schema/person/cf0a25e052dcfc8f60458228f18e5da4","name":"NSS","image":{"@type":"ImageObject","@id":"https://www.analyticsvidhya.com/#personlogo","inLanguage":"en-US","url":"https://secure.gravatar.com/avatar/7a65509c42ede4442b3c88e7d10cd924?s=96&d=mm&r=g","caption":"NSS"},"description":"I am a perpetual, quick learner and keen to explore the realm of Data analytics and science. I am deeply excited about the times we live in and the rate at which data is being generated and being transformed as an asset. I am well versed with a few tools for dealing with data and also in the process of learning some other tools and knowledge required to exploit data."}]}</script>

<link rel="dns-prefetch" href="https://www.analyticsvidhya.com/">
<link rel="dns-prefetch" href="https://cdnjs.cloudflare.com/">
<link rel="dns-prefetch" href="https://s.w.org/">
<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/13.0.0\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/13.0.0\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/www.analyticsvidhya.com\/wp-includes\/js\/wp-emoji-release.min.js?ver=5.5"}};
			!function(e,a,t){var r,n,o,i,p=a.createElement("canvas"),s=p.getContext&&p.getContext("2d");function c(e,t){var a=String.fromCharCode;s.clearRect(0,0,p.width,p.height),s.fillText(a.apply(this,e),0,0);var r=p.toDataURL();return s.clearRect(0,0,p.width,p.height),s.fillText(a.apply(this,t),0,0),r===p.toDataURL()}function l(e){if(!s||!s.fillText)return!1;switch(s.textBaseline="top",s.font="600 32px Arial",e){case"flag":return!c([127987,65039,8205,9895,65039],[127987,65039,8203,9895,65039])&&(!c([55356,56826,55356,56819],[55356,56826,8203,55356,56819])&&!c([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]));case"emoji":return!c([55357,56424,8205,55356,57212],[55357,56424,8203,55356,57212])}return!1}function d(e){var t=a.createElement("script");t.src=e,t.defer=t.type="text/javascript",a.getElementsByTagName("head")[0].appendChild(t)}for(i=Array("flag","emoji"),t.supports={everything:!0,everythingExceptFlag:!0},o=0;o<i.length;o++)t.supports[i[o]]=l(i[o]),t.supports.everything=t.supports.everything&&t.supports[i[o]],"flag"!==i[o]&&(t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&t.supports[i[o]]);t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&!t.supports.flag,t.DOMReady=!1,t.readyCallback=function(){t.DOMReady=!0},t.supports.everything||(n=function(){t.readyCallback()},a.addEventListener?(a.addEventListener("DOMContentLoaded",n,!1),e.addEventListener("load",n,!1)):(e.attachEvent("onload",n),a.attachEvent("onreadystatechange",function(){"complete"===a.readyState&&t.readyCallback()})),(r=t.source||{}).concatemoji?d(r.concatemoji):r.wpemoji&&r.twemoji&&(d(r.twemoji),d(r.wpemoji)))}(window,document,window._wpemojiSettings);
		</script><script src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/wp-emoji-release.min.js" type="text/javascript" defer=""></script>
<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
<link rel="stylesheet" id="jetpack_related-posts-css" href="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/related-posts.css" type="text/css" media="all">
<link rel="stylesheet" id="contact-form-7-css" href="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/styles.css" type="text/css" media="all">
<style id="contact-form-7-inline-css" type="text/css">
.wpcf7 .wpcf7-recaptcha iframe {margin-bottom: 0;}.wpcf7 .wpcf7-recaptcha[data-align="center"] > div {margin: 0 auto;}.wpcf7 .wpcf7-recaptcha[data-align="right"] > div {margin: 0 0 0 auto;}
</style>
<link rel="stylesheet" id="dgd-scrollbox-plugin-core-css" href="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/style.css" type="text/css" media="all">
<link rel="stylesheet" id="stbCoreCSS-css" href="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/stb-core.css" type="text/css" media="all">
<link rel="stylesheet" id="wpss-style-css" href="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/wpss-pkg.min.css" type="text/css" media="all">
<link rel="stylesheet" id="wpss-custom-db-style-css" href="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/custom-performance.css" type="text/css" media="all">
<script type="text/javascript" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/jquery.js" id="jquery-js"></script>
<script type="text/javascript" id="jetpack_related-posts-js-extra">
/* <![CDATA[ */
var related_posts_js_options = {"post_heading":"h4"};
/* ]]> */
</script>
<script type="text/javascript" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/related-posts.min.js" defer="defer" id="jetpack_related-posts-js"></script>
<script type="text/javascript" id="dgd-scrollbox-plugin-js-extra">
/* <![CDATA[ */
var $DGD = {"ajaxurl":"\/wp-admin\/admin-ajax.php","nonce":"e7f598c46b","debug":"","permalink":"https:\/\/www.analyticsvidhya.com\/blog\/2017\/06\/word-embeddings-count-word2veec\/","title":"An Intuitive Understanding of  Word Embeddings: From Count Vectors to Word2Vec","thumbnail":"https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2017\/06\/06062705\/Word-Vectors.png","scripthost":"\/wp-content\/plugins\/dreamgrow-scroll-triggered-box\/","scrollboxes":[{"trigger":{"action":"scroll","scroll":"0","delaytime":"10000","element":""},"vpos":"center","hpos":"center","cookieLifetime":"0","hide_mobile":"1","receiver_email":"0","thankyou":"","submit_auto_close":"5","hide_submitted":"1","delay_auto_close":"0","lightbox":{"enabled":"1","color":"#000000","opacity":"0.7","blur":"2"},"theme":"none","widget_enabled":"1","height":"500","width":"800","jsCss":{"padding":"0","margin":"0","backgroundColor":"","boxShadow":"0px","borderColor":"","borderWidth":"0px","borderRadius":"0px","backgroundImageUrl":""},"closeImageUrl":"","transition":{"from":"b","effect":"none","speed":"400"},"social":{"facebook":"","twitter":"","google":"","pinterest":"","stumbleupon":"","linkedin":""},"id":"dgd_scrollbox-67322","mode":"stb","voff":0,"hoff":0}]};
/* ]]> */
</script>
<script type="text/javascript" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/script.js" defer="defer" id="dgd-scrollbox-plugin-js"></script>
<script type="text/javascript" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/wpss-pkg.min.js" defer="defer" id="wpss-pkg-js"></script>
<link rel="https://api.w.org/" href="https://www.analyticsvidhya.com/wp-json/"><link rel="alternate" type="application/json" href="https://www.analyticsvidhya.com/wp-json/wp/v2/posts/36027"><link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://www.analyticsvidhya.com/xmlrpc.php?rsd">
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://www.analyticsvidhya.com/wp-includes/wlwmanifest.xml">
<meta name="generator" content="WordPress 5.5">
<link rel="shortlink" href="https://www.analyticsvidhya.com/?p=36027">
<link rel="alternate" type="application/json+oembed" href="https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fwww.analyticsvidhya.com%2Fblog%2F2017%2F06%2Fword-embeddings-count-word2veec%2F">
<link rel="alternate" type="text/xml+oembed" href="https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fwww.analyticsvidhya.com%2Fblog%2F2017%2F06%2Fword-embeddings-count-word2veec%2F&amp;format=xml">
<script type="text/javascript"><!--
function powerpress_pinw(pinw_url){window.open(pinw_url, 'PowerPressPlayer','toolbar=0,status=0,resizable=1,width=460,height=320');	return false;}
//-->
</script>
<script type="text/javascript">/* <![CDATA[ */ jQuery.post("https://www.analyticsvidhya.com/wp-admin/admin-ajax.php", { action: "wmp_update", id: 36027, token: "f0e0150bf2" }); /* ]]> */</script>
<style type="text/css">



                #yith-topbar-countdown.topbar-countdown-container{

                    
                                            background-color: #ee3d24;
                    
                    
                    background-repeat:repeat;
                    background-position:top left;
                    background-attachment:scroll;
                }

                #yith-topbar-countdown .countdown_slogan{
                    color: #FFFFFF;
font-size: 14px;
font-family: 'Roboto';
font-weight: 400;
font-style: normal;
                }

                #yith-topbar-countdown .countdown_information .countdown .num {color: #ffffff;
font-size: 14px;
font-family: 'Roboto';
font-weight: 700;
font-style: normal;
}

                #yith-topbar-countdown .countdown_information .countdown_slogan strong{
                    color: #ffffff;
font-size: 14px;
font-family: 'Roboto';
font-weight: 700;
font-style: normal;
                }

                #yith-topbar-countdown .countdown_information .message,
                #yith-topbar-countdown .countdown_information .countdown-label{
                    color: #FFFFFF;
font-size: 14px;
font-family: 'Roboto';
font-weight: 400;
font-style: normal;
                }

                #yith-topbar-countdown .countdown_button {

                                            background-color: #ee3d24;
                    
                    color: #FFFFFF;
font-size: 14px;
font-family: 'Roboto';
font-weight: 700;
font-style: normal;
                }

                #yith-topbar-countdown .countdown_button a{color: #FFFFFF;
font-size: 14px;
font-family: 'Roboto';
font-weight: 700;
font-style: normal;
}

                #yith-topbar-countdown .countdown_button:hover {
                                            background-color: #821919;
                                    }

            </style>

<meta property="og:url" content="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/">
<meta property="og:title" content="An Intuitive Understanding of  Word Embeddings: From Count Vectors to Word2Vec">
<meta property="og:description" content="Introduction Before we start, have a look at the below examples.   	You open Google and search for a news article on the ongoing Champions trophy and get hundreds of search results in return about it.  	Nate silver analysed millions of tweets and correctly predicted the results of 49 out of 50 states">
<meta property="og:image" content="https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/06062705/Word-Vectors-200x200.png">

<link rel="canonical" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/">
<style type="text/css"></style>


<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-MPSM42V');</script>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-40101183-1', 'auto');
  ga('require', 'GTM-PBZGQJ4');
  //ga('send', 'pageview');
  ga('send', 'pageview', {
     'dimension1': 'NSS',
     'dimension2': 'Advanced'
     });
</script>
<meta property="fb:pages" content="452065408218678">
<script src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/jquery.min.js"></script>
<script type="application/javascript" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/app.js" async=""></script>
<script>
    var truepush = window.truepush || [];
    truepush.push(function(){
      truepush.Init({
        id: "5f2968e91e8ca92c6cc55c46"
      },
        function(error){
        if(error) console.log(error);
})
    })
</script>
<link rel="stylesheet" href="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/css"><script type="text/javascript" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/output.9e9961c79503.js"></script></head>
<body class="post-template-default single single-post postid-36027 single-format-standard mh-body chrome">

<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MPSM42V"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>

<div class="sale-strip" id="sale-strip" style="">
<a id="href_id" href="https://courses.analyticsvidhya.com/bundles/certified-machine-learning-master-s-program-mlmp?utm_source=datahack&amp;utm_medium=flashstrip&amp;utm_campaign=MLMP_launch">[Special Launch Offer] Save INR 9000 ($145) on Certified Machine Learning Master's Program | Offer Ending Soon</a>
<div id="clockdiv" style="display: none;">
<span>
<h4 class="days" id="day">NaN</h4>
<sup>D</sup>
</span>
<span>
<h4 class="hours" id="hour">NaN</h4>
<sup>H</sup>
</span>
<span>
<h4 class="minutes" id="minute">NaN</h4>
<sup>M</sup>
</span>
<span>
<h4 class="seconds" id="second">NaN</h4>
<sup>S</sup>
</span>
</div>
</div>
<div id="body-maha" class="body-maha" data-tmloader="https://www.analyticsvidhya.com/wp-content/themes/Curated/images/ellipsis.gif">
<nav id="mobile-bar-sticky" role="navigation" class="mobile-bar bar-sticky">
<div id="close-mobile-bar"><i class="tm-cancel"></i></div>
<div id="search-mobile-bar"><form action="https://www.analyticsvidhya.com/" class="searchform" method="get">
<button class="search-button"><i class="tm-search"></i></button>
<input type="text" name="s" class="search-input" value="Search" onfocus="if(this.value==&#39;Search&#39;)this.value=&#39;&#39;;" onblur="if(this.value==&#39;&#39;)this.value=&#39;Search&#39;;" autocomplete="off">
</form></div>
<ul id="menu-main-menu" class="menu"><li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children mh-navcat-0"><span class="navmob-sub-menu"><i class="tm-dropdown"></i></span><a href="https://www.analyticsvidhya.com/blog/?utm_source=home_blog_navbar">Blog</a>
<ul class="sub-menu">
<li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children mh-navcat-0"><span class="navmob-sub-menu"><i class="tm-dropdown"></i></span><a href="https://www.analyticsvidhya.com/blog-archive/">Blog Archive</a>
<ul class="sub-menu">
<li class="menu-item menu-item-type-taxonomy menu-item-object-category current-post-ancestor current-menu-parent current-post-parent mh-navcat-2498"><span></span><a href="https://www.analyticsvidhya.com/blog/category/machine-learning/">Machine Learning</a></li>
<li class="menu-item menu-item-type-taxonomy menu-item-object-category current-post-ancestor current-menu-parent current-post-parent mh-navcat-3761"><span></span><a href="https://www.analyticsvidhya.com/blog/category/deep-learning/">Deep Learning</a></li>
<li class="menu-item menu-item-type-taxonomy menu-item-object-category mh-navcat-4121"><span></span><a href="https://www.analyticsvidhya.com/blog/category/career/">Career</a></li>
<li class="menu-item menu-item-type-taxonomy menu-item-object-category mh-navcat-3118"><span></span><a href="https://www.analyticsvidhya.com/blog/category/stories/">Stories</a></li>
<li class="menu-item menu-item-type-taxonomy menu-item-object-category mh-navcat-0"><span></span><a href="https://www.analyticsvidhya.com/blog/category/podcast/">DataHack Radio</a></li>
</ul>
</li>
<li class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children mh-navcat-0"><span class="navmob-sub-menu"><i class="tm-dropdown"></i></span><a href="https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/">Learning Paths</a>
<ul class="sub-menu">
<li class="menu-item menu-item-type-post_type menu-item-object-page mh-navcat-0"><span></span><a href="https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/">SAS Business Analyst</a></li>
<li class="menu-item menu-item-type-post_type menu-item-object-page mh-navcat-0"><span></span><a href="https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/">LeaRn Data Science on R</a></li>
<li class="menu-item menu-item-type-post_type menu-item-object-page mh-navcat-0"><span></span><a href="https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/">Data Science in Python</a></li>
<li class="menu-item menu-item-type-post_type menu-item-object-page mh-navcat-0"><span></span><a href="https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/">DATA SCIENCE IN WEKA</a></li>
<li class="menu-item menu-item-type-post_type menu-item-object-page mh-navcat-0"><span></span><a href="https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/">Data Visualization with Tableau</a></li>
<li class="menu-item menu-item-type-post_type menu-item-object-page mh-navcat-0"><span></span><a href="https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/">Data Visualization with QlikView</a></li>
<li class="menu-item menu-item-type-post_type menu-item-object-page mh-navcat-0"><span></span><a href="https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/">Interactive Data Stories with D3.js</a></li>
</ul>
</li>
<li class="menu-item menu-item-type-custom menu-item-object-custom mh-navcat-0"><span></span><a href="https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/">Glossary</a></li>
</ul>
</li>
<li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children mh-navcat-5288"><span class="navmob-sub-menu"><i class="tm-dropdown"></i></span><a href="https://courses.analyticsvidhya.com/">Courses</a>
<ul class="sub-menu">
<li class="menu-item menu-item-type-custom menu-item-object-custom mh-navcat-0"><span></span><a href="https://courses.analyticsvidhya.com/courses/applied-machine-learning-beginner-to-professional?utm_source=home_blog_navbar">Applied Machine Learning â Beginner to Professional</a></li>
<li class="menu-item menu-item-type-custom menu-item-object-custom mh-navcat-0"><span></span><a href="https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2?utm_source=home_blog_navbar">INTRODUCTION TO DATA SCIENCE</a></li>
<li class="menu-item menu-item-type-custom menu-item-object-custom mh-navcat-0"><span></span><a href="https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp?utm_source=home_blog_navbar">Natural Language Processing (NLP) Using Python</a></li>
<li class="menu-item menu-item-type-custom menu-item-object-custom mh-navcat-0"><span></span><a href="https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning-version2?utm_source=home_blog_navbar">Computer Vision using Deep Learning 2.0</a></li>
<li class="menu-item menu-item-type-custom menu-item-object-custom mh-navcat-0"><span></span><a href="https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar">More Courses</a></li>
</ul>
</li>
<li class="menu-item menu-item-type-custom menu-item-object-custom mh-navcat-0"><span></span><a href="https://datahack.analyticsvidhya.com/contest/all">Hackathons</a></li>
<li class="menu-item menu-item-type-custom menu-item-object-custom mh-navcat-170"><span></span><a href="https://jobs.analyticsvidhya.com/?utm_source=home_blog_navbar">Jobs</a></li>
<li class="menu-item menu-item-type-custom menu-item-object-custom mh-navcat-0"><span></span><a href="https://courses.analyticsvidhya.com/bundles/certified-ai-ml-blackbelt-plus/?utm-source=blog-navbar&amp;utm-medium=web">AI &amp; ML BLACKBELT+</a></li>
<li class="menu-item menu-item-type-custom menu-item-object-custom mh-navcat-0"><span></span><a href="https://www.analyticsvidhya.com/ascend-pro/?utm_source=blog&amp;utm_medium=navbar&amp;utm_campaign=ascend_pro"><span style="color:#EE3D24">Ascend Pro</span></a></li>
<li class="menu-item menu-item-type-post_type menu-item-object-page mh-navcat-0"><span></span><a href="https://www.analyticsvidhya.com/contact/">Contact</a></li>
</ul> </nav>

<div id="body-background">

<div id="off-canvas-body" class="off-canvas-body animati-on" style="margin-top: 54px;">

<div id="top-bar-sticky" class="bar-sticky">
<div class="top-bar">

<div class="container">
<div class="row">
<div class="col-sm-12">
<div id="top-right-nav">
<div class="social-top">
<ul><li><a target="_blank" href="https://www.facebook.com/AnalyticsVidhya"><i class="tm-facebook"></i></a></li><li><a target="_blank" href="https://twitter.com/analyticsvidhya"><i class="tm-twitter"></i></a></li><li><a target="_blank" href="https://plus.google.com/+Analyticsvidhya/posts"><i class="tm-googleplus"></i></a></li><li><a target="_blank" href="https://in.linkedin.com/company/analytics-vidhya"><i class="tm-linkedin"></i></a></li></ul>
</div>
<div class="top-user">
<i class="tm-user"></i>
<a class="login-modal-closer" data-reveal-id="cur-register" data-animation="fade" data-dismissmodalclass="register-reveal-modal" id="login1"><a href="https://id.analyticsvidhya.com/auth/login/?next=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/" target="_blank"> Login / Register</a></a><span id="regiter1"></span>
</div>
</div>
<nav id="top-nav-wrapper" class="ul-nav">
<ul id="menu-top" class="menu"><li id="menu-item-42575" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-42575 mh-navcat-0"><span></span><a href="https://www.analyticsvidhya.com/">Home</a></li>
<li id="menu-item-64144" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-64144 mh-navcat-0"><span></span><a href="https://www.analyticsvidhya.com/myfeed/?utm-source=blog&amp;utm-medium=top-icon/">My Feed</a></li>
<li id="menu-item-12992" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-12992 mh-navcat-0"><span></span><a href="https://www.analyticsvidhya.com/blog-archive/">Blog Archive</a></li>
<li id="menu-item-12952" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-12952 mh-navcat-0"><span></span><a href="https://discuss.analyticsvidhya.com/">Discuss</a></li>
<li id="menu-item-39605" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-39605 mh-navcat-0"><span></span><a href="https://www.analyticsvidhya.com/corporate/">Corporate</a></li>
</ul> </nav>
<nav id="top-mobile-wrapper" class="ul-nav">
<ul><li><a><i class="tm-menu"></i></a></li></ul>
</nav>
</div>
</div>
</div>

</div>
</div>


<div class="main-logo-ads-wrap">
<div class="main-logo-ads">

<div class="container">
<div class="row ">
<div class="col-sm-12">

<div id="thelogo" class="logo ">
<a href="https://www.analyticsvidhya.com/blog/">
<img alt="Analytics Vidhya - Learn everything about Analytics" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/Av-Logo-250x71-1.webp" data-retina="https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/Av-Logo-250x71-1.jpg" data-first="https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/Av-Logo-250x71-1.jpg">
</a>
</div>
<h2 class="site-outline">Learn everything about Analytics</h2>

<div id="main-ads" class="main-ads-right">
<div><a href="https://www.analyticsvidhya.com/back-channel/download-starter-kit.php?utm_source=ml-interview-guide&amp;id=10"><img src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/Machine-Learning-Interview-Guide.webp"></a></div>
</div>
</div>
</div>
</div>

</div>
</div>


<div id="main-nav">
<div id="main-nav-bar" class="main-nav-bar clearfix on-stuck" style="margin-top: 0px;">
<div id="main-search" class=" clearfix">
<div id="con-search" style="top: 54px;">
<div class="container">
<div class="cols-sm12">
<div id="main-search-form">
<span class="close-search-form"><i class="tm-cancel"></i></span>
<span class="loading-search-result"><img src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/ellipsis.gif" alt=""></span>
<form action="https://www.analyticsvidhya.com/" class="searchform" method="get">
<button class="search-button"><i class="tm-search"></i></button>
<input type="text" name="s" class="search-input" value="Search" onfocus="if(this.value==&#39;Search&#39;)this.value=&#39;&#39;;" onblur="if(this.value==&#39;&#39;)this.value=&#39;Search&#39;;" autocomplete="off">
</form> <div class="search-result">
<div class="container search-result-content"></div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="maha-notification container"><div id="main-notif"></div></div>
<div class="container clearfix">
</div>

<div class="container clearfix">
<div class="row clearfix">
<div class="col-sm-12 clearfix">


<div id="search-nav" class="search-nav clearfix">
<span class="open-search-form"><i class="tm-search"></i></span>
</div>


<nav id="main-nav-wrapper" class="main-ul-nav clearfix">
<div id="thelogosmall" class="logo nav-main-affix" style="width: 168px;">
<a href="https://www.analyticsvidhya.com/blog/">
<img alt="Analytics Vidhya - Learn everything about Analytics" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/Av-logo-138x40-1.webp" data-retina="https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/Av-logo-319x90-1.jpg" data-first="https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/Av-logo-138x40-1.jpg">
</a>
</div>
<ul id="menu-main-menu-1" class="menu"><li id="menu-item-37781" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children mh-navcat-0 ord-nav-offset"><a href="https://www.analyticsvidhya.com/blog/?utm_source=home_blog_navbar">Blog<span class="sub-ord-nav"> <i class="tm-dropdown"></i> </span></a><div class="nav-sub-wrap ord-nav" style="display: none; opacity: 0;"><div class="nsw clearfix">
<div class="nav-sub-menus"><ul>
<li id="menu-item-55484" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children mh-navcat-0 ord-nav-offset"><a href="https://www.analyticsvidhya.com/blog-archive/">Blog Archive<span class="sub-ord-nav"> <i class="tm-right-open-mini"></i> </span></a><div class="nav-sub-wrap ord-nav" style="display: none; opacity: 0;"><div class="nsw clearfix">
<div class="nav-sub-menus"><ul>
<li id="menu-item-47251" class="menu-item menu-item-type-taxonomy menu-item-object-category current-post-ancestor current-menu-parent current-post-parent mh-navcat-2498"><a href="https://www.analyticsvidhya.com/blog/category/machine-learning/">Machine Learning</a></li>
<li id="menu-item-47265" class="menu-item menu-item-type-taxonomy menu-item-object-category current-post-ancestor current-menu-parent current-post-parent mh-navcat-3761"><a href="https://www.analyticsvidhya.com/blog/category/deep-learning/">Deep Learning</a></li>
<li id="menu-item-47191" class="menu-item menu-item-type-taxonomy menu-item-object-category mh-navcat-4121"><a href="https://www.analyticsvidhya.com/blog/category/career/">Career</a></li>
<li id="menu-item-27174" class="menu-item menu-item-type-taxonomy menu-item-object-category mh-navcat-3118"><a href="https://www.analyticsvidhya.com/blog/category/stories/">Stories</a></li>
<li id="menu-item-45209" class="menu-item menu-item-type-taxonomy menu-item-object-category mh-navcat-0"><a href="https://www.analyticsvidhya.com/blog/category/podcast/">DataHack Radio</a></li>
</ul></div>
</div></div>
</li>
<li id="menu-item-12170" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children mh-navcat-0 ord-nav-offset"><a href="https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/">Learning Paths<span class="sub-ord-nav"> <i class="tm-right-open-mini"></i> </span></a><div class="nav-sub-wrap ord-nav" style="display: none; opacity: 0;"><div class="nsw clearfix">
<div class="nav-sub-menus"><ul>
<li id="menu-item-12503" class="menu-item menu-item-type-post_type menu-item-object-page mh-navcat-0"><a href="https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/">SAS Business Analyst</a>
</li>
<li id="menu-item-14416" class="menu-item menu-item-type-post_type menu-item-object-page mh-navcat-0"><a href="https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/">LeaRn Data Science on R</a>
</li>
<li id="menu-item-12171" class="menu-item menu-item-type-post_type menu-item-object-page mh-navcat-0"><a href="https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/">Data Science in Python</a>
</li>
<li id="menu-item-13207" class="menu-item menu-item-type-post_type menu-item-object-page mh-navcat-0"><a href="https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/">DATA SCIENCE IN WEKA</a>
</li>
<li id="menu-item-23077" class="menu-item menu-item-type-post_type menu-item-object-page mh-navcat-0"><a href="https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/">Data Visualization with Tableau</a>
</li>
<li id="menu-item-12320" class="menu-item menu-item-type-post_type menu-item-object-page mh-navcat-0"><a href="https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/">Data Visualization with QlikView</a>
</li>
<li id="menu-item-23078" class="menu-item menu-item-type-post_type menu-item-object-page mh-navcat-0"><a href="https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/">Interactive Data Stories with D3.js</a>
</li>
</ul></div>
</div></div>
</li>
<li id="menu-item-55487" class="menu-item menu-item-type-custom menu-item-object-custom mh-navcat-0"><a href="https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/">Glossary</a>
</li>
</ul></div>
</div></div>
</li>
<li id="menu-item-44643" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children mh-navcat-5288 ord-nav-offset"><a href="https://courses.analyticsvidhya.com/">Courses<span class="sub-ord-nav"> <i class="tm-dropdown"></i> </span></a><div class="nav-sub-wrap ord-nav" style="display: none; opacity: 0;"><div class="nsw clearfix">
<div class="nav-sub-menus"><ul>
<li id="menu-item-53978" class="menu-item menu-item-type-custom menu-item-object-custom mh-navcat-0"><a href="https://courses.analyticsvidhya.com/courses/applied-machine-learning-beginner-to-professional?utm_source=home_blog_navbar">Applied Machine Learning â Beginner to Professional</a>
</li>
<li id="menu-item-44644" class="menu-item menu-item-type-custom menu-item-object-custom mh-navcat-0"><a href="https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2?utm_source=home_blog_navbar">INTRODUCTION TO DATA SCIENCE</a>
</li>
<li id="menu-item-48965" class="menu-item menu-item-type-custom menu-item-object-custom mh-navcat-0"><a href="https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp?utm_source=home_blog_navbar">Natural Language Processing (NLP) Using Python</a>
</li>
<li id="menu-item-48964" class="menu-item menu-item-type-custom menu-item-object-custom mh-navcat-0"><a href="https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning-version2?utm_source=home_blog_navbar">Computer Vision using Deep Learning 2.0</a>
</li>
<li id="menu-item-44646" class="menu-item menu-item-type-custom menu-item-object-custom mh-navcat-0"><a href="https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar">More Courses</a>
</li>
</ul></div>
</div></div>
</li>
<li id="menu-item-24638" class="menu-item menu-item-type-custom menu-item-object-custom mh-navcat-0"><a href="https://datahack.analyticsvidhya.com/contest/all">Hackathons</a>
</li>
<li id="menu-item-55521" class="menu-item menu-item-type-custom menu-item-object-custom mh-navcat-170"><a href="https://jobs.analyticsvidhya.com/?utm_source=home_blog_navbar">Jobs</a>
</li>
<li id="menu-item-37784" class="menu-item menu-item-type-custom menu-item-object-custom mh-navcat-0"><a href="https://courses.analyticsvidhya.com/bundles/certified-ai-ml-blackbelt-plus/?utm-source=blog-navbar&amp;utm-medium=web">AI &amp; ML BLACKBELT+</a>
</li>
<li id="menu-item-69325" class="menu-item menu-item-type-custom menu-item-object-custom mh-navcat-0"><a href="https://www.analyticsvidhya.com/ascend-pro/?utm_source=blog&amp;utm_medium=navbar&amp;utm_campaign=ascend_pro"><span style="color:#EE3D24">Ascend Pro</span></a>
</li>
<li id="menu-item-24641" class="menu-item menu-item-type-post_type menu-item-object-page mh-navcat-0"><a href="https://www.analyticsvidhya.com/contact/">Contact</a>
</li>
</ul>
</nav>
</div>
</div>
</div>

</div>
</div>


<div class="page-wrapper">

<div class="mh-el page-sidebar single-regular" itemscope="" itemtype="https://schema.org/Article">

<div class="container">
<div class="row">

<div class="col-sm-8">
<div class="main-content">

<div class="maha-crumbs"><span><span><a href="https://www.analyticsvidhya.com/">Home</a> Â» <span class="breadcrumb_last" aria-current="page">An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec</span></span></span></div>
<article id="post-36027" class="main-content single-post-box">
<header>
<meta itemprop="headline" content="An Intuitive Understanding of  Word Embeddings: From Count Vectors to Word2Vec">
<meta itemprop="datecreated" content="2017-06-04T23:15:40+05:30">
<meta itemprop="datePublished" content="2017-06-04T23:15:40+05:30">
<meta itemprop="dateModified" content="2020-01-20T15:49:00+05:30">
<meta itemscope="" itemprop="mainEntityOfPage" itemtype="https://schema.org/WebPage" itemid="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/">
<span class="hidden" itemprop="image" itemscope="" itemtype="https://schema.org/ImageObject">
<meta itemprop="url" content="https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/06062705/Word-Vectors.png">
<meta itemprop="width" content="527">
<meta itemprop="height" content="527">
</span>
<span class="hidden" itemprop="publisher" itemscope="" itemtype="https://schema.org/Organization">
<span class="hidden" itemprop="logo" itemscope="" itemtype="https://schema.org/ImageObject">
<meta itemprop="url" content="https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/Av-Logo-250x71-1.jpg">
</span>
<meta itemprop="name" content="Analytics Vidhya">
</span>
<div class="meta-count">
<span class="i-category"><span class="mh-cat-item"><a class="mh-elcat-5384" href="https://www.analyticsvidhya.com/blog/category/advanced/" title="Advanced" rel="category">Advanced</a></span><span class="mh-cat-item"><a class="mh-elcat-5283" href="https://www.analyticsvidhya.com/blog/category/algorithm/" title="Algorithm" rel="category">Algorithm</a></span><span class="mh-cat-item"><a class="mh-elcat-3761" href="https://www.analyticsvidhya.com/blog/category/deep-learning/" title="Deep Learning" rel="category">Deep Learning</a></span><span class="mh-cat-item"><a class="mh-elcat-2498" href="https://www.analyticsvidhya.com/blog/category/machine-learning/" title="Machine Learning" rel="category">Machine Learning</a></span><span class="mh-cat-item"><a class="mh-elcat-3952" href="https://www.analyticsvidhya.com/blog/category/nlp/" title="NLP" rel="category">NLP</a></span><span class="mh-cat-item"><a class="mh-elcat-396" href="https://www.analyticsvidhya.com/blog/category/python-2/" title="Python" rel="category">Python</a></span><span class="mh-cat-item"><a class="mh-elcat-5307" href="https://www.analyticsvidhya.com/blog/category/technique/" title="Technique" rel="category">Technique</a></span><span class="mh-cat-item"><a class="mh-elcat-5364" href="https://www.analyticsvidhya.com/blog/category/text/" title="Text" rel="category">Text</a></span><span class="mh-cat-item"><a class="mh-elcat-5365" href="https://www.analyticsvidhya.com/blog/category/unstructured-data/" title="Unstructured Data" rel="category">Unstructured Data</a></span></span>
</div>
<h1 itemprop="name" class="entry-title">An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec</h1>
<div class="meta-info">
<span class="entry-author">
<a href="https://www.analyticsvidhya.com/blog/author/nss/"><span>NSS</span></a>,
</span>
<time class="entry-date" content="2017-06-04" datetime="2017-06-04T23:15:40+05:30">
June 4, 2017 </time>
<div class="count-data right">
</div>
<span class="save-for-later"><a href="https://id.analyticsvidhya.com/auth/login/?next=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/" target="_blank">Login to Bookmark this article</a></span>
<span class="download-pdf"><a href="https://www.analyticsvidhya.com/back-channel/download-pdf.php?pid=36027" target="_blank"><img src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/download-post-pdf.webp" alt="Click to Download PDF" title="Click to Download PDF of this Article"></a></span>
</div>
</header>
<div class="text-content">
<div id="ban3_av"><a href="https://ascendpro.analyticsvidhya.com/?utm_source=blog&amp;utm_medium=banner_below_blog_title&amp;utm_campaign=Ascend_pro_launch" target="_blank"><img src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/Ascend-Pro.webp"></a></div>
<h2>Introduction</h2>
<p>Before we start, have a look at the below examples.</p>
<ol>
<li>You open Google and search for a news article on the ongoing Champions trophy and get hundreds of search results in return about it.</li>
<li>Nate silver analysed millions of tweets and correctly predicted the results of 49 out of 50 states in 2008 U.S Presidential Elections.</li>
<li>You type a sentence in google translate in English and get an Equivalent Chinese conversion.</li>
</ol>
<p><img loading="lazy" class="aligncenter wp-image-36152 size-full" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/collage.webp" alt="" width="927" height="93" srcset="https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/06052154/collage.png 927w, https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/06052154/collage-300x30.png 300w, https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/06052154/collage-768x77.png 768w, https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/06052154/collage-850x85.png 850w" sizes="(max-width: 927px) 100vw, 927px"></p>
<p>&nbsp;</p>
<p>So what do the above examples have in common?</p>
<p>You possible guessed it right â <strong>TEXT processing</strong>. All the above three scenarios deal with humongous amount of text to perform different range of tasks like clustering in the google search example, classification in the second and Machine Translation in the third.</p>
<p>Humans can deal with text format quite intuitively but provided we have millions of documents being generated in a single day, we cannot have humans performing the above the three tasks. It is neither scalable nor effective.</p>
<p><img loading="lazy" class="size-medium wp-image-36158 aligncenter" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/joke-297x300.jpg" alt="" width="297" height="300" srcset="https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/06064148/joke-297x300.jpg 297w, https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/06064148/joke-83x83.jpg 83w, https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/06064148/joke.jpg 448w" sizes="(max-width: 297px) 100vw, 297px"></p>
<p>So, how do we make computers of today perform clustering, classification etc on a text data since we know that they are generally inefficient at handling and processing strings or texts for any fruitful outputs?</p>
<p>Sure, a computer can match two strings and tell you whether they are same or not. But how do we make computers tell you about football or Ronaldo when you search for Messi? How do you make a computer understand that âAppleâ in âApple is a tasty fruitâ is a fruit that can be eaten and not a company?</p>
<p>The answer to the above questions lie in creating a representation for words that capture their <em>meanings</em>, <em>semantic relationships</em> and the different types of contexts they are used in.</p>
<p>And all of these are implemented by using Word Embeddings or numerical representations of texts so that computers may handle them.</p>
<p>Below, we will see formally what are Word Embeddings and their different types and how we can actually implement them to perform the tasks like returning efficient Google search results.</p>
<p>&nbsp;</p>
<table border="1">
<tbody>
<tr>
<td>
<h2 style="text-align: center;"><span style="text-decoration: underline;">Project to apply Word Embeddings for Text Classification</span></h2>
<h2><b>Problem Statement</b></h2>
<p>The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.</p>
<p>Formally, given a training sample of tweets and labels, where label â1â denotes the tweet is racist/sexist and label â0â denotes the tweet is not racist/sexist, your objective is to predict the labels on the test dataset.</p>
<p style="text-align: center;"><span style="font-weight: 400;"><a style="text-decoration: none; width: 20%; padding: 10px; cursor: pointer; font-weight: bold; font-size: 130%; background: #3366cc; color: #fff; border: 1px solid #3366cc; border-radius: 10px;" href="https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/?utm_source=av_blog&amp;utm_medium=practice_blog_word_embeddings" target="_blank" rel="noopener noreferrer">Practice Now</a></span></p>
</td>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<h2>Table of Contents</h2>
<ol>
<li>What are Word Embeddings?</li>
<li>Different types of Word Embeddings<br>
2.1 &nbsp;Frequency based Embedding<br>
2.1.1 Count Vectors<br>
2.1.2 TF-IDF<br>
2.1.3 Co-Occurrence Matrix<br>
2.2 &nbsp;Prediction based Embedding<br>
2.2.1 CBOW<br>
2.2.2 Skip-Gram</li>
<li>Word Embeddings use case scenarios(what all can be done using word embeddings? eg: similarity, odd one out etc.)</li>
<li>Using pre-trained Word Vectors</li>
<li>Training your own Word Vectors</li>
<li>End Notes</li>
</ol>
<p>&nbsp;</p>
<h2>1. What are Word Embeddings?</h2>
<p>In very simplistic terms, Word Embeddings are the texts converted into numbers and there may be different numerical representations of the same text. But before we dive into the details of Word Embeddings, the following question should be asked â Why do we need Word Embeddings?</p>
<p>As it turns out, many Machine Learning algorithms and almost all Deep Learning Architectures are incapable of processing <em>strings&nbsp;</em>or <em>plain text&nbsp;</em>in their raw form. They require numbers as inputs to perform any sort of job, be it classification, regression etc. in broad terms. And with the huge amount of data that is present in the text format, it is imperative to extract knowledge out of it and build applications. Some real world applications of text applications are â sentiment analysis of reviews by Amazon etc., document or news classification or clustering by Google&nbsp;etc.</p>
<p>Let us now define Word Embeddings formally. A Word Embedding format generally tries to map a word using a dictionary to a vector. Let us break this sentence down into finer details to have a clear view.</p>
<p>Take a look at this example â <strong>sentence</strong>=â Word Embeddings are Word converted into numbers&nbsp;â</p>
<p>A <em>word&nbsp;</em>in this <strong>sentence</strong> may be âEmbeddingsâ or ânumbers â etc.</p>
<p>A <em>dictionary&nbsp;</em>may be the list of all unique words in the <strong>sentence.&nbsp;</strong>So, a dictionary may look like â [âWordâ,âEmbeddingsâ,âareâ,âConvertedâ,âintoâ,ânumbersâ]</p>
<p>A ve<em>ctor </em>representation of a word&nbsp;may be a one-hot encoded vector where 1 stands for the position where the word exists and 0 everywhere else. The vector representation of ânumbersâ<strong>&nbsp;</strong>in this format according to the above dictionary is [0,0,0,0,0,1] and of converted is[0,0,0,1,0,0].</p>
<p>This is just a very simple method to represent a word in the vector form. Let us look at different types of Word Embeddings or Word Vectors and their advantages and disadvantages over the rest.</p>
<p>&nbsp;</p>
<h2>2. Different types of Word Embeddings</h2>
<p>The different types of word embeddings can be broadly classified into two categories-</p>
<ol>
<li>Frequency based Embedding</li>
<li>Prediction based Embedding</li>
</ol>
<p>Let us try to understand each of these methods in detail.</p>
<p>&nbsp;</p>
<h3>2.1 Frequency based Embedding</h3>
<p>There are generally three types of vectors that we encounter under this category.</p>
<ol>
<li>Count Vector</li>
<li>TF-IDF Vector</li>
<li>Co-Occurrence Vector</li>
</ol>
<p>Let us look into each of these&nbsp;vectorization methods in detail.</p>
<p>&nbsp;</p>
<h4>2.1.1 Count Vector</h4>
<p>Consider a Corpus C of D documents {d1,d2â¦..dD} and N unique tokens extracted out of the corpus C. The N tokens will form our dictionary and the size of the Count Vector matrix M will be given by D X N. Each row in the matrix M contains the frequency of tokens in document D(i).</p>
<p>Let us understand this using a simple example.</p>
<p>D1: He is a lazy boy. She is also lazy.</p>
<p>D2: Neeraj is a lazy person.</p>
<p>The dictionary created may be a list of unique tokens(words) in the corpus =[âHeâ,âSheâ,âlazyâ,âboyâ,âNeerajâ,âpersonâ]</p>
<p>Here, D=2, N=6</p>
<p>The count matrix M of size 2 X 6 will be represented as â</p>
<table width="448">
<tbody>
<tr>
<td width="64"></td>
<td width="64">He</td>
<td width="64">She</td>
<td width="64">lazy</td>
<td width="64">boy</td>
<td width="64">Neeraj</td>
<td width="64">person</td>
</tr>
<tr>
<td>D1</td>
<td>1</td>
<td>1</td>
<td>2</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>D2</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>Now, a column can also be understood as word vector for the corresponding word in the matrix M. For example, the word vector for âlazyâ in the above matrix is [2,1] and so on.Here, the <em>rows</em> correspond to the <em>documents</em> in the corpus and the <em>columns</em> correspond to the <em>tokens</em> in the dictionary. The second row in the above matrix may be read as â D2 contains âlazyâ: once, âNeerajâ: once and âpersonâ once.</p>
<p>Now there may be quite a few variations while preparing the above matrix M. The variations will be generally in-</p>
<ol>
<li>The way dictionary is prepared.<br>
Why? Because in real world applications we might have a corpus which contains millions of documents. And with millions of document, we can extract hundreds of millions of unique words. So basically, the matrix that will be prepared like above will be a very sparse one and inefficient for any computation. So an alternative to using every unique word as a dictionary element would be to pick say top 10,000 words based on frequency and then prepare a dictionary.</li>
<li>The way count is taken for each word.<br>
We may either take the frequency (number of times a word has appeared in the document) or the presence(has the word appeared in the document?) to be the entry in the count matrix M. But generally, frequency method is preferred over the latter.</li>
</ol>
<p>Below is a representational image of the matrix M for easy understanding.</p>
<p>&nbsp;</p>
<p><img loading="lazy" class="aligncenter wp-image-36035 size-full" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/count-vector.webp" alt="" width="372" height="276" srcset="https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/04164920/count-vector.png 372w, https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/04164920/count-vector-300x223.png 300w, https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/04164920/count-vector-83x63.png 83w" sizes="(max-width: 372px) 100vw, 372px"></p>
<p>&nbsp;</p>
<h4>2.1.2 TF-IDF vectorization</h4>
<p>This is another method which is based on the frequency method but it is different to the count vectorization in the sense that it takes into account not just the occurrence of a word in a single document but in the entire corpus. So, what is the rationale behind this? Let us try to understand.</p>
<p>Common words like âisâ, âtheâ, âaâ etc. tend to appear quite frequently in comparison to the words which are important to a document. For example, a document <strong>A</strong> on Lionel Messi is going to contain more occurences of the word âMessiâ in comparison to other documents. But common words like âtheâ etc. are also going to be present in higher frequency in almost every document.</p>
<p>Ideally, what we would want is to down weight the common words occurring in almost all documents and give more importance to words that appear in a subset of documents.</p>
<p>TF-IDF works by penalising these common words by assigning them lower weights while giving importance to words like Messi in a particular document.</p>
<p>So, how exactly does TF-IDF work?</p>
<p>Consider the below sample table which gives the count of terms(tokens/words) in two documents.</p>
<p><img loading="lazy" class="wp-image-36037 size-full aligncenter" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/Tf-IDF.webp" alt="" width="494" height="208" srcset="https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/04171138/Tf-IDF.png 494w, https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/04171138/Tf-IDF-300x126.png 300w" sizes="(max-width: 494px) 100vw, 494px"></p>
<p>Now, let us define a few terms related to TF-IDF.</p>
<p>&nbsp;</p>
<p>TF = (Number of times term t appears in a document)/(Number of terms in the document)</p>
<p>So, TF(This,Document1) = 1/8</p>
<p>TF(This, Document2)=1/5</p>
<p>It denotes the contribution of the word to the document i.e words relevant to the document should be frequent. eg: A document about Messi should contain the word âMessiâ in large number.</p>
<p>IDF = log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.</p>
<p>where N is the number of documents and n is the number of documents a term t has appeared in.</p>
<p>So, IDF(This) = log(2/2) = 0.</p>
<p>So, how do we explain the reasoning behind IDF? Ideally, if a word has appeared in all the document, then probably that word is not relevant to a particular document. But if it has appeared in a subset of documents then probably the word is of some relevance to the documents it is present in.</p>
<p>Let us compute IDF for the word âMessiâ.</p>
<p>IDF(Messi) = log(2/1) = 0.301.</p>
<p>Now, let us compare the TF-IDF for a common word âThisâ and a word âMessiâ which seems to be of relevance to Document 1.</p>
<p>TF-IDF(This,Document1) = (1/8) * (0) = 0</p>
<p>TF-IDF(This, Document2) = (1/5) * (0) = 0</p>
<p>TF-IDF(Messi, Document1) =&nbsp;(4/8)*0.301 = 0.15</p>
<p>As, you can see for Document1 , TF-IDF method heavily penalises the word âThisâ but assigns greater weight to âMessiâ. So, this may be understood as âMessiâ is an important word for Document1 from the context of the entire corpus.</p>
<p>&nbsp;</p>
<h4>2.1.3 Co-Occurrence Matrix with a fixed context window</h4>
<p><strong>The big idea</strong> â Similar words tend to occur together and will have similar context for example â Apple is a fruit. Mango is a fruit.<br>
Apple and mango tend to have a similar context i.e fruit.</p>
<p>Before I dive into the details of how a co-occurrence matrix is constructed, there are two concepts that need to be clarified â Co-Occurrence and Context Window.</p>
<p>Co-occurrence â For a given corpus, the co-occurrence of a pair of words say w1 and w2 is the number of times they have appeared together in a Context Window.</p>
<p>Context Window â Context window is specified by a number and the direction. So what does a context window of 2 (around) means? Let us see an example below,</p>
<p>&nbsp;</p>
<table border="0" cellspacing="0">
<colgroup span="8" width="85"></colgroup>
<tbody>
<tr>
<td style="text-align: center;" align="left" bgcolor="#66FF99" height="17">Quick</td>
<td style="text-align: center;" align="left" bgcolor="#66FF99"><span style="color: #000000;">Brown</span></td>
<td style="text-align: center;" align="left" bgcolor="#FF6600"><span style="color: #000000;">Fox</span></td>
<td style="text-align: center;" align="left" bgcolor="#66FF99">Jump</td>
<td style="text-align: center;" align="left" bgcolor="#66FF99">Over</td>
<td style="text-align: center;" align="left">The</td>
<td style="text-align: center;" align="left">Lazy</td>
<td style="text-align: center;" align="left">Dog</td>
</tr>
</tbody>
</table>
<p>The green words are a 2 (around) context window for the word âFoxâ and for calculating the co-occurrence only these words will be counted. Let us see context window for the word âOverâ.</p>
<p>&nbsp;</p>
<table border="0" cellspacing="0">
<colgroup span="8" width="85"></colgroup>
<tbody>
<tr>
<td style="text-align: center;" align="left" height="17">Quick</td>
<td style="text-align: center;" align="left">Brown</td>
<td style="text-align: center;" align="left" bgcolor="#66FF99">Fox</td>
<td style="text-align: center;" align="left" bgcolor="#66FF99">Jump</td>
<td style="text-align: center;" align="left" bgcolor="#FF6600">Over</td>
<td style="text-align: center;" align="left" bgcolor="#66FF99">The</td>
<td style="text-align: center;" align="left" bgcolor="#66FF99">Lazy</td>
<td style="text-align: center;" align="left">Dog</td>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<p>Now, let us take an example corpus to calculate a co-occurrence matrix.</p>
<p>Corpus = He is not lazy. He is intelligent. He is smart.</p>
<p>&nbsp;</p>
<table border="0" cellspacing="0">
<colgroup span="7" width="85"></colgroup>
<tbody>
<tr>
<td align="center" bgcolor="#CCFF00" height="17"><b>&nbsp;</b></td>
<td align="center" bgcolor="#CCFF00"><b>He</b></td>
<td align="center" bgcolor="#CCFF00"><b>is</b></td>
<td align="center" bgcolor="#CCFF00"><b>not</b></td>
<td align="center" bgcolor="#CCFF00"><b>lazy</b></td>
<td align="center" bgcolor="#CCFF00"><b>intelligent</b></td>
<td align="center" bgcolor="#CCFF00"><b>smart</b></td>
</tr>
<tr>
<td align="center" bgcolor="#CCFF00" height="17"><b>He</b></td>
<td align="center" bgcolor="#CCCCCC">0</td>
<td align="center" bgcolor="#FF3333">4</td>
<td align="center" bgcolor="#CCCCCC">2</td>
<td align="center" bgcolor="#CCCCCC">1</td>
<td align="center" bgcolor="#CCCCCC">2</td>
<td align="center" bgcolor="#CCCCCC">1</td>
</tr>
<tr>
<td align="center" bgcolor="#CCFF00" height="17"><b>is</b></td>
<td align="center" bgcolor="#CCCCCC">4</td>
<td align="center" bgcolor="#CCCCCC">0</td>
<td align="center" bgcolor="#CCCCCC">1</td>
<td align="center" bgcolor="#CCCCCC">2</td>
<td align="center" bgcolor="#CCCCCC">2</td>
<td align="center" bgcolor="#CCCCCC">1</td>
</tr>
<tr>
<td align="center" bgcolor="#CCFF00" height="17"><b>not</b></td>
<td align="center" bgcolor="#CCCCCC">2</td>
<td align="center" bgcolor="#CCCCCC">1</td>
<td align="center" bgcolor="#CCCCCC">0</td>
<td align="center" bgcolor="#CCCCCC">1</td>
<td align="center" bgcolor="#CCCCCC">0</td>
<td align="center" bgcolor="#CCCCCC">0</td>
</tr>
<tr>
<td align="center" bgcolor="#CCFF00" height="17"><b>lazy</b></td>
<td align="center" bgcolor="#CCCCCC">1</td>
<td align="center" bgcolor="#CCCCCC">2</td>
<td align="center" bgcolor="#CCCCCC">1</td>
<td align="center" bgcolor="#CCCCCC">0</td>
<td align="center" bgcolor="#00CCFF">0</td>
<td align="center" bgcolor="#CCCCCC">0</td>
</tr>
<tr>
<td align="center" bgcolor="#CCFF00" height="17"><b>intelligent</b></td>
<td align="center" bgcolor="#CCCCCC">2</td>
<td align="center" bgcolor="#CCCCCC">2</td>
<td align="center" bgcolor="#CCCCCC">0</td>
<td align="center" bgcolor="#CCCCCC">0</td>
<td align="center" bgcolor="#CCCCCC">0</td>
<td align="center" bgcolor="#CCCCCC">0</td>
</tr>
<tr>
<td align="center" bgcolor="#CCFF00" height="17"><b>smart</b></td>
<td align="center" bgcolor="#CCCCCC">1</td>
<td align="center" bgcolor="#CCCCCC">1</td>
<td align="center" bgcolor="#CCCCCC">0</td>
<td align="center" bgcolor="#CCCCCC">0</td>
<td align="center" bgcolor="#CCCCCC">0</td>
<td align="center" bgcolor="#CCCCCC">0</td>
</tr>
</tbody>
</table>
<p>Let us understand this co-occurrence matrix by seeing two examples in the table above. Red and the blue box.</p>
<p>Red box- It is the number of times âHeâ and âisâ have appeared in the context window 2 and it can be seen that the count turns out to be 4. The below table will help you visualise the count.</p>
<table border="0" cellspacing="0">
<colgroup span="10" width="85"></colgroup>
<tbody>
<tr>
<td align="left" bgcolor="#CC9900" height="17">He</td>
<td align="left" bgcolor="#CC9900">is</td>
<td align="left">not</td>
<td align="left">lazy</td>
<td align="left">He</td>
<td align="left">is</td>
<td align="left">intelligent</td>
<td align="left">He</td>
<td align="left">is</td>
<td align="left">smart</td>
</tr>
<tr>
<td align="left" height="17"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left" height="17">He</td>
<td align="left">is</td>
<td align="left">not</td>
<td align="left">lazy</td>
<td align="left" bgcolor="#CC9900">He</td>
<td align="left" bgcolor="#CC9900">is</td>
<td align="left">intelligent</td>
<td align="left">He</td>
<td align="left">is</td>
<td align="left">smart</td>
</tr>
<tr>
<td align="left" height="17"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left" height="17">He</td>
<td align="left">is</td>
<td align="left">not</td>
<td align="left">lazy</td>
<td align="left">He</td>
<td align="left" bgcolor="#CC9900">is</td>
<td align="left">intelligent</td>
<td align="left" bgcolor="#CC9900">He</td>
<td align="left">is</td>
<td align="left">smart</td>
</tr>
<tr>
<td align="left" height="17"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left" height="17">He</td>
<td align="left">is</td>
<td align="left">not</td>
<td align="left">lazy</td>
<td align="left">He</td>
<td align="left">is</td>
<td align="left">intelligent</td>
<td align="left" bgcolor="#CC9900">He</td>
<td align="left" bgcolor="#CC9900">is</td>
<td align="left">smart</td>
</tr>
</tbody>
</table>
<p>while the word âlazyâ has never appeared with âintelligentâ in the context window and therefore has been assigned 0 in the blue box.</p>
<p>&nbsp;</p>
<p><strong>Variations of Co-occurrence Matrix</strong></p>
<p>Letâs say there are V unique words in the corpus. So Vocabulary size = V. The columns of the Co-occurrence matrix form the <em>context word</em>s. The different variations of Co-Occurrence Matrix are-</p>
<ol>
<li>A co-occurrence matrix of size V X V. Now, for even a decent corpus V gets very large and difficult to handle. So generally, this architecture is never preferred in practice.</li>
<li>A co-occurrence matrix of size V X N where N is a subset of V and can be obtained by removing irrelevant words like stopwords etc. for example. This is still very large and presents computational difficulties.</li>
</ol>
<p>But, remember this co-occurrence matrix is not the word vector representation that is generally used. Instead, this Co-occurrence matrix is decomposed using techniques like PCA, SVD etc. into factors and combination of these factors forms the word vector representation.</p>
<p>Let me illustrate this more clearly. For example, you perform PCA on the above matrix of size VXV. You will obtain V principal components. You can choose k components out of these V components. So, the new matrix will be of the form V X k.</p>
<p>And, a single word, instead of being represented in V dimensions will be represented in k dimensions while still capturing almost the same semantic meaning. k is generally of the order of hundreds.</p>
<p>So, what PCA does at the back is decompose Co-Occurrence matrix into three matrices, U,S and V where U and V are both orthogonal matrices. What is of importance is that dot product of U and S gives the word vector representation and V gives the word context representation.</p>
<p><img loading="lazy" class="aligncenter wp-image-36054 size-full" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/svd2.webp" alt="" width="1515" height="275" srcset="https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/04224842/svd2.png 1515w, https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/04224842/svd2-300x54.png 300w, https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/04224842/svd2-768x139.png 768w, https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/04224842/svd2-850x154.png 850w" sizes="(max-width: 1515px) 100vw, 1515px"></p>
<p>&nbsp;</p>
<p><strong>Advantages of Co-occurrence Matrix</strong></p>
<ol>
<li>It preserves the semantic relationship between words. i.e man and woman tend to be closer than man and apple.</li>
<li>It uses SVD at its core, which produces more accurate word vector representations than existing methods.</li>
<li>It uses factorization which is a well-defined problem and can be efficiently solved.</li>
<li>It has to be computed once and can be used anytime once computed. In this sense, it is faster in comparison to others.</li>
</ol>
<p>&nbsp;</p>
<p><strong>Disadvantages of Co-Occurrence Matrix</strong></p>
<ol>
<li>It requires huge memory to store the co-occurrence matrix.<br>
But, this problem can be circumvented by factorizing the matrix out of the system for example in Hadoop clusters etc. and can be saved.</li>
</ol>
<p>&nbsp;</p>
<h2>2.2 Prediction based Vector</h2>
<p><strong>Pre-requisite</strong>: This section assumes that you have a working knowledge of how a neural network works and the mechanisms by which weights in an NN are updated. If you are new to Neural Network, I would suggest you go through <a href="https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/" target="_blank" rel="noopener noreferrer">this awesome article</a> by Sunil to gain a very good understanding of how NN works.</p>
<p>So far, we have seen deterministic methods to determine word vectors. But these methods proved to be limited in their word representations until Mitolov etc. el introduced word2vec to the NLP community. These methods were prediction based in the sense that they provided probabilities to the words and proved to be state of the art for tasks like word analogies and word similarities. They were also able to achieve tasks like King -man +woman = Queen, which was considered a result almost magical. So let us look at the word2vec model used as of today to generate word vectors.</p>
<p>Word2vec is not a single algorithm but a combination of two techniques â CBOW(Continuous bag of words) and Skip-gram model. Both of these are shallow neural networks which map word(s) to the target variable which is also a word(s). Both of these techniques learn weights which act as word vector representations. Let us discuss both these methods separately and gain intuition into their working.</p>
<p>&nbsp;</p>
<h3>2.2.1 CBOW (Continuous Bag of words)</h3>
<p>The way CBOW work is that it tends to predict the probability of a word given a context. A context may be a single word or a group of words. But for simplicity, I will take a single context word and try to predict a single target word.</p>
<p>Suppose, we have a corpus C = âHey, this is sample corpus using only one context word.â and we have defined a context window of 1. This corpus may be converted into a training set for a CBOW model as follow. The input is shown below. The matrix on the right in the below image contains the one-hot encoded from of the input on the left.</p>
<p><img loading="lazy" class="aligncenter wp-image-36044 size-full" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/cbow1.webp" alt="" width="1150" height="325" srcset="https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/04205949/cbow1.png 1150w, https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/04205949/cbow1-300x85.png 300w, https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/04205949/cbow1-768x217.png 768w, https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/04205949/cbow1-850x240.png 850w" sizes="(max-width: 1150px) 100vw, 1150px"></p>
<p>The target for a single datapoint say Datapoint 4 is shown as below</p>
<table width="640">
<tbody>
<tr>
<td width="64">Hey</td>
<td width="64">this</td>
<td width="64">is</td>
<td width="64">sample</td>
<td width="64">corpus</td>
<td width="64">using</td>
<td width="64">only</td>
<td width="64">one</td>
<td width="64">context</td>
<td width="64">word</td>
</tr>
<tr>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<p>This matrix shown in the above image is sent into a shallow neural network with three layers: an&nbsp;input layer, a hidden layer and an output layer. The output layer is a softmax layer which is used to sum the probabilities obtained in the output layer to 1. Now let us see how the forward propagation will work to calculate the hidden layer activation.</p>
<p>Let us first see a diagrammatic representation of the CBOW model.</p>
<p><img loading="lazy" class="aligncenter wp-image-36053 size-full" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/Screenshot-from-2017-06-04-22-40-29.webp" alt="" width="411" height="253" srcset="https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/04224109/Screenshot-from-2017-06-04-22-40-29.png 411w, https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/04224109/Screenshot-from-2017-06-04-22-40-29-300x185.png 300w" sizes="(max-width: 411px) 100vw, 411px"></p>
<p>The matrix representation of the above image for a single data point is below.</p>
<p><img loading="lazy" class="aligncenter wp-image-36051 size-full" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/Screenshot-from-2017-06-04-22-19-202.webp" alt="" width="1507" height="227" srcset="https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/04222108/Screenshot-from-2017-06-04-22-19-202.png 1507w, https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/04222108/Screenshot-from-2017-06-04-22-19-202-300x45.png 300w, https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/04222108/Screenshot-from-2017-06-04-22-19-202-768x116.png 768w, https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/04222108/Screenshot-from-2017-06-04-22-19-202-850x128.png 850w" sizes="(max-width: 1507px) 100vw, 1507px"></p>
<p>The flow is as follows:</p>
<ol>
<li>The input layer and the target, both are one- hot encoded of size [1 X V]. Here V=10 in the above example.</li>
<li>There are two sets of weights. one is between the input and the hidden layer and second between hidden and output layer.<br>
Input-Hidden layer matrix size =[V X N] , hidden-Output layer matrix &nbsp;size =[N X V] : Where N is the number of dimensions we choose to represent our word in. It is arbitary&nbsp;and a hyper-parameter for a Neural Network. Also, N is the number of neurons in the hidden layer. Here, N=4.</li>
<li>There is a no activation function between any layers.( More specifically, I am referring to linear activation)</li>
<li>The input is multiplied by the input-hidden weights and called hidden activation. It is simply the corresponding row in the input-hidden matrix copied.</li>
<li>The hidden input gets multiplied by hidden- output weights and output is calculated.</li>
<li>Error between output and target is calculated and propagated back to re-adjust the weights.</li>
<li>The weight &nbsp;between the hidden layer and the output layer is taken as the word vector representation of the word.</li>
</ol>
<p>We saw the above steps for a single context word. Now, what about if we have multiple context words? The image below describes the architecture for multiple context words.</p>
<p><img loading="lazy" class="size-medium wp-image-36046 aligncenter" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/Screenshot-from-2017-06-04-22-05-44-261x300.png" alt="" width="261" height="300" srcset="https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/04220606/Screenshot-from-2017-06-04-22-05-44-261x300.png 261w, https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/04220606/Screenshot-from-2017-06-04-22-05-44.png 330w" sizes="(max-width: 261px) 100vw, 261px"></p>
<p>Below is a matrix representation of the above architecture for an easy understanding.</p>
<p><img loading="lazy" class="aligncenter wp-image-36048 size-full" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/Screenshot-from-2017-06-04-22-14-311.webp" alt="" width="1475" height="216" srcset="https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/04221550/Screenshot-from-2017-06-04-22-14-311.png 1475w, https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/04221550/Screenshot-from-2017-06-04-22-14-311-300x44.png 300w, https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/04221550/Screenshot-from-2017-06-04-22-14-311-768x112.png 768w, https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/04221550/Screenshot-from-2017-06-04-22-14-311-850x124.png 850w" sizes="(max-width: 1475px) 100vw, 1475px"></p>
<p>The image above takes 3 context words and predicts the probability of a target word. The input can be assumed as taking three one-hot encoded vectors in the input layer as shown above in red, blue and green.</p>
<p>So, the input layer will have 3 [1 X V] Vectors in the input as shown above and 1 [1 X V] in the output layer. Rest of the architecture is same as for a 1-context CBOW.</p>
<p>The steps remain the same, only the calculation of hidden activation changes. Instead of just copying the corresponding rows of the input-hidden weight matrix to the hidden layer, an average is taken over all the corresponding rows of the matrix. We can understand this with the above figure. The average vector calculated becomes the hidden activation. So, if we have three context words for a single target word, we will have three initial hidden activations which are then averaged element-wise to obtain the final activation.</p>
<p>In both a single context word and multiple context word, I have shown the images till the calculation of the hidden activations since this is the part where CBOW differs from a simple MLP network. The steps after the calculation of hidden layer are same as that of the MLP as mentioned in this article â <a href="https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/" target="_blank" rel="noopener noreferrer">Understanding and Coding Neural Networks from scratch</a>.</p>
<p>The differences between MLP and CBOW are &nbsp;mentioned below for clarification:</p>
<ol>
<li>The objective function in MLP is a MSE(mean square error) whereas in CBOW it is negative log likelihood of a word given a set of context i.e -log(p(wo/wi)), where p(wo/wi) is given as</li>
</ol>
<p><img loading="lazy" class="size-medium wp-image-36055 aligncenter" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/AAEAAQAAAAAAAA18AAAAJGNkMGYxMDIxLWY5NjgtNGEzMy1hMjAyLWU4MmI4ZWUwNDNhYw-300x91.jpg" alt="" width="300" height="91" srcset="https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/04230048/AAEAAQAAAAAAAA18AAAAJGNkMGYxMDIxLWY5NjgtNGEzMy1hMjAyLWU4MmI4ZWUwNDNhYw-300x91.jpg 300w, https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/04230048/AAEAAQAAAAAAAA18AAAAJGNkMGYxMDIxLWY5NjgtNGEzMy1hMjAyLWU4MmI4ZWUwNDNhYw-768x233.jpg 768w, https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/04230048/AAEAAQAAAAAAAA18AAAAJGNkMGYxMDIxLWY5NjgtNGEzMy1hMjAyLWU4MmI4ZWUwNDNhYw-850x258.jpg 850w, https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/04230048/AAEAAQAAAAAAAA18AAAAJGNkMGYxMDIxLWY5NjgtNGEzMy1hMjAyLWU4MmI4ZWUwNDNhYw.jpg 2000w" sizes="(max-width: 300px) 100vw, 300px"></p>
<p style="padding-left: 60px;">wo : output word<br>
wi: context words</p>
<p style="padding-left: 30px;">2. The gradient of error with respect to hidden-output weights and input-hidden weights are different since MLP has &nbsp;sigmoid activations(generally) but CBOW has linear activations. The method however to calculate the gradient is same as an MLP.</p>
<p>&nbsp;</p>
<p><strong>Advantages of CBOW:</strong></p>
<ol>
<li>Being probabilistic is nature, it is supposed to perform superior to deterministic methods(generally).</li>
<li>It is low on memory. It does not need to have huge RAM requirements like that of co-occurrence matrix where it needs to store three huge matrices.</li>
</ol>
<p>&nbsp;</p>
<p><strong>Disadvantages of CBOW:</strong></p>
<ol>
<li>CBOW takes the average of the context of a word (as seen above in calculation of hidden activation). For example, Apple can be both a fruit and a company but CBOW takes an average of both the contexts and places it in between a cluster for fruits and companies.</li>
<li>Training a CBOW from scratch can take forever if not properly optimized.</li>
</ol>
<p>&nbsp;</p>
<h2>2.2.2 Skip â Gram model</h2>
<p>Skip â gram follows the same topology as of CBOW. It just flips CBOWâs architecture on its head. The aim of skip-gram is to predict the context given a word. Let us take the same corpus that we built our CBOW model on. C=âHey, this is sample corpus using only one context word.â Let us construct the training data.</p>
<p><img loading="lazy" class="size-medium wp-image-36059 aligncenter" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/Capture1-300x222.png" alt="" width="300" height="222" srcset="https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/04235354/Capture1-300x222.png 300w, https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/04235354/Capture1.png 302w" sizes="(max-width: 300px) 100vw, 300px"></p>
<p>The input vector for skip-gram is going to be similar to a 1-context CBOW model. Also, the calculations up to hidden layer activations are going to be the same. The difference will be in the target variable. Since we have defined a context window of 1 on both the sides, there will be â<strong>twoâ one hot encoded target variables</strong> and â<strong>twoâ corresponding outputs</strong> as can be seen by the blue section in the image.</p>
<p>Two separate errors are calculated with respect to the two target variables and the two error vectors obtained are added element-wise to obtain a final error vector which is propagated back to update the weights.</p>
<p>The weights between the input and the hidden layer are taken as the word vector representation after training. The loss function or the objective is of the same type as of the CBOW model.</p>
<p>The skip-gram architecture is shown below.</p>
<p><img loading="lazy" class="size-medium wp-image-36060 aligncenter" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/Capture2-276x300.webp" alt="" width="276" height="300"></p>
<p>&nbsp;</p>
<p>For a better understanding, matrix style structure with calculation has been shown below.</p>
<p><img loading="lazy" class="aligncenter wp-image-36079 size-full" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/skip.webp" alt="" width="1220" height="266" srcset="https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/05122225/skip.png 1220w, https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/05122225/skip-300x65.png 300w, https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/05122225/skip-768x167.png 768w, https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/05122225/skip-850x185.png 850w" sizes="(max-width: 1220px) 100vw, 1220px"></p>
<p>&nbsp;</p>
<p>Let us break down the above image.</p>
<p>Input layer &nbsp;size â [1 X V], Input hidden weight matrix size â [V X N], Number of neurons in hidden layer â N, Hidden-Output weight matrix size â [N X V], Output layer size â C [1 X V]</p>
<p>In the above example, C is the number of context words=2, V= 10, N=4</p>
<ol>
<li>The row in red is the hidden activation corresponding to the input one-hot encoded vector. It is basically the corresponding row of input-hidden matrix copied.</li>
<li>The yellow matrix is the weight between the hidden layer and the output layer.</li>
<li>The blue matrix is obtained by the matrix multiplication of hidden activation and the hidden output weights. There will be two rows calculated for two target(context) words.</li>
<li>Each row of the blue matrix is converted into its softmax probabilities individually as shown in the green box.</li>
<li>The grey matrix contains the one hot encoded vectors of the two context words(target).</li>
<li>Error is calculated by substracting the first row of the grey matrix(target) from the first row of the green matrix(output) element-wise. This is repeated for the next row. Therefore, for <strong>n&nbsp;</strong>target context words, we will have <strong>n </strong>error vectors.</li>
<li>Element-wise sum is taken over all the error vectors to obtain a final error vector.</li>
<li>This error vector is propagated back to update the weights.</li>
</ol>
<h3>Advantages of Skip-Gram Model</h3>
<ol>
<li>Skip-gram model can capture two semantics for a single word. i.e it will have two vector representations of Apple. One for the company and other for the fruit.</li>
<li>Skip-gram with negative sub-sampling outperforms every other method generally.</li>
</ol>
<p>&nbsp;</p>
<p><a href="http://bit.ly/wevi-online" target="_blank" rel="noopener noreferrer">This</a>&nbsp;is an excellent interactive tool to visualise CBOW and skip gram in action. I would suggest you to really go through this link for a better understanding.</p>
<h2>3. Word Embeddings use case scenarios</h2>
<p>Since word embeddings or word Vectors are numerical representations of contextual similarities between words, they can be manipulated and made to perform amazing tasks like-</p>
<ol>
<li>Finding the degree of similarity between two words.<br>
<code>model.similarity('woman','man')</code><br>
<code>0.73723527</code></li>
<li>Finding odd one out.<br>
<code>model.doesnt_match('breakfast cereal dinner lunch';.split())</code><br>
<code>'cereal'</code></li>
<li>Amazing things like woman+king-man =queen<br>
<code>model.most_similar(positive=['woman','king'],negative=['man'],topn=1)</code><br>
<code>queen: 0.508</code></li>
<li>Probability of a text under the model<br>
<code>model.score(['The fox jumped over the lazy dog'.split()])</code><br>
<code>0.21</code></li>
</ol>
<p style="padding-left: 30px;">Below is one interesting visualisation of word2vec.</p>
<p><img loading="lazy" class="size-medium wp-image-36062 aligncenter" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/graph1-300x277.jpg" alt="" width="300" height="277" srcset="https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/05003425/graph1-300x277.jpg 300w, https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/05003425/graph1.jpg 600w" sizes="(max-width: 300px) 100vw, 300px"></p>
<p style="padding-left: 30px;">The above image is a t-SNE representation of word vectors in 2 dimension and you can see that two contexts of apple have been captured. One is a fruit and the other company.</p>
<p>5. &nbsp;It can be used to perform Machine Translation.<br>
<img loading="lazy" class="size-medium wp-image-36064 aligncenter" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/ml-300x211.png" alt="" width="300" height="211" srcset="https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/05003807/ml-300x211.png 300w, https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/05003807/ml.png 354w" sizes="(max-width: 300px) 100vw, 300px"></p>
<p>The above graph is a bilingual embedding with chinese in green and english in yellow. If we know the words having similar meanings in chinese and english, the above bilingual embedding can be used to translate one language into the other.</p>
<p>&nbsp;</p>
<h2>4. Using pre-trained word vectors</h2>
<p>We are going to use googleâs pre-trained model. It contains word vectors for a vocabulary of 3 million words trained on around 100 billion words from the google news dataset. The downlaod link for the model is <a href="https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit">this</a>. Beware it is a 1.5 GB download.</p>
<p><code>from </code>gensim<code>.models import Word2Vec<br>
</code></p>
<p><code>#loading the downloaded model</code><br>
<code>model <span class="pl-k">=</span> Word2Vec.load_word2vec_format(<span class="pl-s"><span class="pl-pds">'</span>GoogleNews-vectors-negative300.bin<span class="pl-pds">'</span></span>, <span class="pl-v">binary</span><span class="pl-k">=</span><span class="pl-c1">True</span>, <span class="pl-v">norm_only</span><span class="pl-k">=</span><span class="pl-c1">True</span>)</code></p>
<p><code>#the model is loaded. It can be used to perform all of the tasks mentioned above.</code></p>
<p><code># getting word vectors of a word</code><br>
<code>dog <span class="pl-k">=</span> model[<span class="pl-s"><span class="pl-pds">'</span>dog<span class="pl-pds">'</span></span>]</code></p>
<p><code>#performing king queen magic</code><br>
<code><span class="pl-c1">print</span>(model.most_similar(<span class="pl-v">positive</span><span class="pl-k">=</span>[<span class="pl-s"><span class="pl-pds">'</span>woman<span class="pl-pds">'</span></span>, <span class="pl-s"><span class="pl-pds">'</span>king<span class="pl-pds">'</span></span>], <span class="pl-v">negative</span><span class="pl-k">=</span>[<span class="pl-s"><span class="pl-pds">'</span>man<span class="pl-pds">'</span></span>]))</code></p>
<p><code>#picking odd one out</code><br>
<code><span class="pl-c1">print</span>(model.doesnt_match(<span class="pl-s"><span class="pl-pds">"</span>breakfast cereal dinner lunch<span class="pl-pds">"</span></span>.split()))</code></p>
<p><code>#printing similarity index</code><br>
<code><span class="pl-c1">print</span>(model.similarity(<span class="pl-s"><span class="pl-pds">'</span>woman<span class="pl-pds">'</span></span>, <span class="pl-s"><span class="pl-pds">'</span>man<span class="pl-pds">'</span></span>))</code></p>
<p>&nbsp;</p>
<h2>5. Training your own word vectors</h2>
<p>We will be training our own word2vec on a custom corpus. For training the model we will be using gensim and the steps are illustrated as below.</p>
<p>word2Vec requires that a format of list of list for training where every document is contained in a list and every list contains list of tokens of that documents. I wonât be covering the pre-preprocessing part here. So letâs take an example list of list to train our word2vec model.</p>
<p>sentence=[[âNeerajâ,âBoyâ],[âSarwanâ,âisâ],[âgoodâ,âboyâ]]</p>
<p><code>#training word2vec on 3 sentences</code><br>
<code class="python plain">model </code><code class="python keyword">=</code> <code class="python plain">gensim.models.Word2Vec(sentence, min_count</code><code class="python keyword">=</code><code class="python value">1,size=300,workers=4</code><code class="python plain">)</code></p>
<p>Let us try to understand the parameters of this model.</p>
<p>sentence â list of list of our corpus<br>
min_count=1 -the threshold value for the words. Word with frequency greater than this only are going to be included into the model.<br>
size=300 â the number of dimensions in which we wish to represent our word. This is the size of the word vector.<br>
workers=4 â used for parallelization</p>
<p><code>#using the model<br>
#The new trained model can be used similar to the pre-trained ones.</code></p>
<p><code>#printing similarity index</code><br>
<code><span class="pl-c1">print</span>(model.similarity(<span class="pl-s"><span class="pl-pds">'</span>woman<span class="pl-pds">'</span></span>, <span class="pl-s"><span class="pl-pds">'</span>man<span class="pl-pds">'</span></span>))</code></p>
<p>&nbsp;</p>
<h2>Projects</h2>
<p>Now, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Accelerate your NLP journey with the following Practice Problems:</p>
<table style="border-collapse: collapse; width: 100%; height: 54px;" border="1">
<tbody>
<tr style="height: 18px;">
<td style="width: 33.3333%; height: 18px;"><a href="https://datahack.analyticsvidhya.com/contest/linguipedia-codefest-natural-language-processing-1/?utm_source=word-embeddings-count-word2veec&amp;utm_medium=blog" target="_blank" rel="noopener noreferrer"><img loading="lazy" class="alignnone wp-image-49933" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/sentiments_1920x480-thumbnail-1200x1200-90.jpg" alt="" width="300" height="150"></a></td>
<td style="width: 33.3333%; height: 18px;"><a href="https://datahack.analyticsvidhya.com/contest/linguipedia-codefest-natural-language-processing-1/?utm_source=word-embeddings-count-word2veec&amp;utm_medium=blog" target="_blank" rel="noopener noreferrer">Practice Problem: Identify the Sentiments</a></td>
<td style="width: 33.3333%; height: 18px;">Identify the sentiment of tweets</td>
</tr>
<tr style="height: 18px;">
<td style="width: 33.3333%; height: 18px;"><a href="https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/?utm_source=word-embeddings-count-word2veec&amp;utm_medium=blog" target="_blank" rel="noopener noreferrer"><img loading="lazy" class="alignnone size-medium wp-image-49934" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/practice_prob_1-thumbnail-1200x1200.png" alt="" width="300" height="150"></a></td>
<td style="width: 33.3333%; height: 18px;"><a href="https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/?utm_source=word-embeddings-count-word2veec&amp;utm_medium=blog" target="_blank" rel="noopener noreferrer">Practice Problem : Twitter Sentiment Analysis</a></td>
<td style="width: 33.3333%; height: 18px;">To detect hate speech in tweets</td>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<h2>6. End Notes</h2>
<p>Word Embeddings is an active research area trying to figure out better word representations than the existing ones. But, with time they have grown large in number and more complex. This article was aimed at simplying some of the workings of these embedding models without carrying the mathematical overhead. If you feel think that I was able to clear some of your confusion, comment below. Any changes or suggestions would be welcomed.</p>
<p>Note: <em>We also have a <a href="https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp?utm_source=blog&amp;utm_medium=word-embeddings-count-word2veec" target="_blank" rel="noopener noreferrer">video course</a> on Natural Language Processing covering many NLP topics including bag of words, TF-IDF, and word embeddings. Do check it out!</em></p>
You can also read this article on our Mobile APP <a href="https://play.google.com/store/apps/details?id=com.analyticsvidhya.android&amp;utm_source=blog_article&amp;utm_campaign=blog&amp;pcampaignid=MKT-Other-global-all-co-prtnr-py-PartBadge-Mar2515-1" onclick="ga(&#39;send&#39;, {&#39;hitType&#39;: &#39;event&#39;, &#39;eventCategory&#39;: &#39;app-blog&#39;, &#39;eventAction&#39;: &#39;clicked&#39;});"><img alt="Get it on Google Play" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/en_badge_web_generic.png" style="height:80px"></a><a href="https://apps.apple.com/us/app/analytics-vidhya/id1470025572"><img src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/download-appstore-apple.webp"></a>
<div id="jp-relatedposts" class="jp-relatedposts">
<h3>Related Articles</h3>
</div><script async="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/f.txt"></script>

<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-5229672700622157" data-ad-slot="7938033629"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
</div>
<footer class="clearfix both">
<div class="meta-tags clearfix">
Tags : <a href="https://www.analyticsvidhya.com/blog/tag/artificial-neural-network/" rel="tag">Artificial Neural Network</a><span>,</span> <a href="https://www.analyticsvidhya.com/blog/tag/deep-learning/" rel="tag">deep learning</a><span>,</span> <a href="https://www.analyticsvidhya.com/blog/tag/machine-learning/" rel="tag">machine learning</a><span>,</span> <a href="https://www.analyticsvidhya.com/blog/tag/natural-language-processing/" rel="tag">Natural language processing</a><span>,</span> <a href="https://www.analyticsvidhya.com/blog/tag/nlp/" rel="tag">NLP</a><span>,</span> <a href="https://www.analyticsvidhya.com/blog/tag/text-processing/" rel="tag">Text Processing</a><span>,</span> <a href="https://www.analyticsvidhya.com/blog/tag/word2vec/" rel="tag">Word2Vec</a> </div>
<div class="next-prev clearfix">
<a class="next" href="https://www.analyticsvidhya.com/blog/2017/06/senior-data-analyst-chennai-2-4-years-of-experience/" rel="next"><i class="tm-chevron-right"></i><div class="np-caption">Next Article</div><h3 class="np-title">Senior Data Analyst-Chennai (2-4 Years of Experience)</h3></a> <i class="tm-3dots"></i>
<a class="prev" href="https://www.analyticsvidhya.com/blog/2017/06/data-science-evangelist-gurgaon-2-to-3-years-of-experience/" rel="prev"><i class="tm-chevron-left"></i><div class="np-caption">Previous Article</div><h3 class="np-title">Data Science Evangelist- Gurgaon (2 to 3 years of experience)</h3></a> </div>
<div class="meta-author" itemprop="author" itemscope="" itemtype="https://schema.org/Person">
<div class="author-thumb">
<a href="https://www.analyticsvidhya.com/blog/author/nss/">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/7a65509c42ede4442b3c88e7d10cd924.jpeg" srcset="https://secure.gravatar.com/avatar/7a65509c42ede4442b3c88e7d10cd924?s=400&amp;d=mm&amp;r=g 2x" class="avatar avatar-200 photo" height="200" width="200" loading="lazy"> </a>
</div>
<div class="author-info">
<h3 class="author-name">
<a itemprop="name" content="NSS" href="https://www.analyticsvidhya.com/blog/author/nss/">NSS</a>
</h3>
<div class="author-desc">
<span class="fn">I am a perpetual, quick learner and keen to explore the realm of Data analytics and science. I am deeply excited about the times we live in and the rate at which data is being generated and being transformed as an asset. I am well versed with a few tools for dealing with data and also in the process of learning some other tools and knowledge required to exploit data.</span>
</div>
<ul class="author-links">
<li><a target="_blank" href="https://in.linkedin.com/in/neeraj-singh-sarwan-a84b6965"><i class="tm-linkedin"></i></a></li> </ul>
</div>
</div>
</footer>
</article> 


<div class="row">
<div class="col-sm-12">
<div class="message-article">This article is quite old and you might not get a prompt response from the author. We request you to post this comment on Analytics Vidhya's <a href="https://discuss.analyticsvidhya.com/"><b>Discussion portal</b></a> to get your queries resolved</div> </div>
</div>
<div class="comment-wrap">

<div class="row">
<div class="col-sm-12">
<div class="block-cap" id="comments">
<h3 id="comment">38 Comments</h3>
</div>
</div>
</div>
<div class="navigation">
<div class="alignleft"></div>
<div class="alignright"></div>
</div>
<ul class="comment-list" id="show-comments">
<li class="comment even thread-even depth-1 parent" id="comment-129869">
<div id="div-comment-129869" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/ff849372158de63b8901f84d5dba417a.png" srcset="https://secure.gravatar.com/avatar/ff849372158de63b8901f84d5dba417a?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">Preeti Agarwal</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129869">
June 6, 2017 at 10:55 am </a>
</div>
<p>Very nicely explainedâ¦ Had read somewhere on tuning the word matrix further â¦ will post the link shortly!!</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129869" data-commentid="129869" data-postid="36027" data-belowelement="div-comment-129869" data-respondelement="respond" data-replyto="Reply to Preeti Agarwal" aria-label="Reply to Preeti Agarwal">Reply</a></div>
</div>
<ul class="children">
<li class="comment byuser comment-author-nss bypostauthor odd alt depth-2" id="comment-130115">
<div id="div-comment-130115" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/7a65509c42ede4442b3c88e7d10cd924(1).jpeg" srcset="https://secure.gravatar.com/avatar/7a65509c42ede4442b3c88e7d10cd924?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">NSS</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130115">
June 9, 2017 at 2:34 pm </a>
</div>
<p>Sure and Thank You.</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130115" data-commentid="130115" data-postid="36027" data-belowelement="div-comment-130115" data-respondelement="respond" data-replyto="Reply to NSS" aria-label="Reply to NSS">Reply</a></div>
</div>
</li>
</ul>
</li>
<li class="comment even thread-odd thread-alt depth-1 parent" id="comment-129871">
<div id="div-comment-129871" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/2075f414ae67891a17f7d9e1aac514bb.png" srcset="https://secure.gravatar.com/avatar/2075f414ae67891a17f7d9e1aac514bb?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">sandip</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129871">
June 6, 2017 at 12:21 pm </a>
</div>
<p>Very nice article</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129871" data-commentid="129871" data-postid="36027" data-belowelement="div-comment-129871" data-respondelement="respond" data-replyto="Reply to sandip" aria-label="Reply to sandip">Reply</a></div>
</div>
<ul class="children">
<li class="comment byuser comment-author-nss bypostauthor odd alt depth-2" id="comment-130116">
<div id="div-comment-130116" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/7a65509c42ede4442b3c88e7d10cd924(1).jpeg" srcset="https://secure.gravatar.com/avatar/7a65509c42ede4442b3c88e7d10cd924?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">NSS</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130116">
June 9, 2017 at 2:34 pm </a>
</div>
<p>Thank You.</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130116" data-commentid="130116" data-postid="36027" data-belowelement="div-comment-130116" data-respondelement="respond" data-replyto="Reply to NSS" aria-label="Reply to NSS">Reply</a></div>
</div>
</li>
</ul>
</li>
<li class="comment even thread-even depth-1 parent" id="comment-129875">
<div id="div-comment-129875" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/b8a847798562a2349686b8e51c968261.png" srcset="https://secure.gravatar.com/avatar/b8a847798562a2349686b8e51c968261?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">ajit balakrishnan</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129875">
June 6, 2017 at 1:25 pm </a>
</div>
<p>excellent summary</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129875" data-commentid="129875" data-postid="36027" data-belowelement="div-comment-129875" data-respondelement="respond" data-replyto="Reply to ajit balakrishnan" aria-label="Reply to ajit balakrishnan">Reply</a></div>
</div>
<ul class="children">
<li class="comment byuser comment-author-nss bypostauthor odd alt depth-2" id="comment-130117">
<div id="div-comment-130117" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/7a65509c42ede4442b3c88e7d10cd924(1).jpeg" srcset="https://secure.gravatar.com/avatar/7a65509c42ede4442b3c88e7d10cd924?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">NSS</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130117">
June 9, 2017 at 2:34 pm </a>
</div>
<p>Thank You.</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130117" data-commentid="130117" data-postid="36027" data-belowelement="div-comment-130117" data-respondelement="respond" data-replyto="Reply to NSS" aria-label="Reply to NSS">Reply</a></div>
</div>
</li>
</ul>
</li>
<li class="comment even thread-odd thread-alt depth-1 parent" id="comment-129880">
<div id="div-comment-129880" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/601d07758b58b6fdadc53fd3eb75acea.png" srcset="https://secure.gravatar.com/avatar/601d07758b58b6fdadc53fd3eb75acea?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">Yousra</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129880">
June 6, 2017 at 3:11 pm </a>
</div>
<p>Very good article. .It helped me to better understand word2vec. .thanks</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129880" data-commentid="129880" data-postid="36027" data-belowelement="div-comment-129880" data-respondelement="respond" data-replyto="Reply to Yousra" aria-label="Reply to Yousra">Reply</a></div>
</div>
<ul class="children">
<li class="comment byuser comment-author-nss bypostauthor odd alt depth-2" id="comment-130114">
<div id="div-comment-130114" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/7a65509c42ede4442b3c88e7d10cd924(1).jpeg" srcset="https://secure.gravatar.com/avatar/7a65509c42ede4442b3c88e7d10cd924?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">NSS</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130114">
June 9, 2017 at 2:33 pm </a>
</div>
<p>Thank You.</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130114" data-commentid="130114" data-postid="36027" data-belowelement="div-comment-130114" data-respondelement="respond" data-replyto="Reply to NSS" aria-label="Reply to NSS">Reply</a></div>
</div>
</li>
</ul>
</li>
<li class="comment even thread-even depth-1 parent" id="comment-129896">
<div id="div-comment-129896" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/6c84c5aba3ed9087d2f57491e3c6c3d0.png" srcset="https://secure.gravatar.com/avatar/6c84c5aba3ed9087d2f57491e3c6c3d0?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">Zach Smith</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129896">
June 6, 2017 at 8:28 pm </a>
</div>
<p>Nice article. Although there is an inconsistency in section 2.1.1. In your written example you say documents = rows and terms = columns, but the visualization of M that you show has that switched. Am I wrong in thinking that the visualization is wrong and that if matters that documents are assigned to rows and terms are assigned to columns?</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129896" data-commentid="129896" data-postid="36027" data-belowelement="div-comment-129896" data-respondelement="respond" data-replyto="Reply to Zach Smith" aria-label="Reply to Zach Smith">Reply</a></div>
</div>
<ul class="children">
<li class="comment byuser comment-author-nss bypostauthor odd alt depth-2" id="comment-129898">
<div id="div-comment-129898" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/7a65509c42ede4442b3c88e7d10cd924(1).jpeg" srcset="https://secure.gravatar.com/avatar/7a65509c42ede4442b3c88e7d10cd924?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">NSS</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129898">
June 6, 2017 at 9:45 pm </a>
</div>
<p>@Zach Smithâ¦. It doesnât matter in this case. The entries i.e the occurrences of terms in a document are still going to be the same.</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129898" data-commentid="129898" data-postid="36027" data-belowelement="div-comment-129898" data-respondelement="respond" data-replyto="Reply to NSS" aria-label="Reply to NSS">Reply</a></div>
</div>
</li>
</ul>
</li>
<li class="comment even thread-odd thread-alt depth-1 parent" id="comment-129987">
<div id="div-comment-129987" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/4bd01ae01955d5b0b8e5a6980ce1bfaf.png" srcset="https://secure.gravatar.com/avatar/4bd01ae01955d5b0b8e5a6980ce1bfaf?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">ZunwenYou</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129987">
June 8, 2017 at 7:07 am </a>
</div>
<p>great article. thank you</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129987" data-commentid="129987" data-postid="36027" data-belowelement="div-comment-129987" data-respondelement="respond" data-replyto="Reply to ZunwenYou" aria-label="Reply to ZunwenYou">Reply</a></div>
</div>
<ul class="children">
<li class="comment byuser comment-author-nss bypostauthor odd alt depth-2" id="comment-130113">
<div id="div-comment-130113" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/7a65509c42ede4442b3c88e7d10cd924(1).jpeg" srcset="https://secure.gravatar.com/avatar/7a65509c42ede4442b3c88e7d10cd924?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">NSS</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130113">
June 9, 2017 at 2:33 pm </a>
</div>
<p>Thank You.</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130113" data-commentid="130113" data-postid="36027" data-belowelement="div-comment-130113" data-respondelement="respond" data-replyto="Reply to NSS" aria-label="Reply to NSS">Reply</a></div>
</div>
</li>
</ul>
</li>
<li class="comment even thread-even depth-1 parent" id="comment-129992">
<div id="div-comment-129992" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/90a90c17bcd80ebf62e6ff586f48053b.png" srcset="https://secure.gravatar.com/avatar/90a90c17bcd80ebf62e6ff586f48053b?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn"><a href="http://wwww.datafuture.wordpress.com/" rel="external nofollow ugc" class="url">Ravi Theja</a></cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129992">
June 8, 2017 at 9:59 am </a>
</div>
<p>Great Article.<br>
Does each word have two vector representationâ¦one representation for word acting as context word and other representation for word acting as central word??</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129992" data-commentid="129992" data-postid="36027" data-belowelement="div-comment-129992" data-respondelement="respond" data-replyto="Reply to Ravi Theja" aria-label="Reply to Ravi Theja">Reply</a></div>
</div>
<ul class="children">
<li class="comment byuser comment-author-nss bypostauthor odd alt depth-2 parent" id="comment-130112">
<div id="div-comment-130112" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/7a65509c42ede4442b3c88e7d10cd924(1).jpeg" srcset="https://secure.gravatar.com/avatar/7a65509c42ede4442b3c88e7d10cd924?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">NSS</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130112">
June 9, 2017 at 2:32 pm </a>
</div>
<p>Each word has just one vector representation. You can use it either as a context or an input. Thanks.</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130112" data-commentid="130112" data-postid="36027" data-belowelement="div-comment-130112" data-respondelement="respond" data-replyto="Reply to NSS" aria-label="Reply to NSS">Reply</a></div>
</div>
<ul class="children">
<li class="comment even depth-3 parent" id="comment-130173">
<div id="div-comment-130173" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/90a90c17bcd80ebf62e6ff586f48053b.png" srcset="https://secure.gravatar.com/avatar/90a90c17bcd80ebf62e6ff586f48053b?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn"><a href="http://www.datafuture.wordpress.com/" rel="external nofollow ugc" class="url">Ravi Theja</a></cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130173">
June 9, 2017 at 11:26 pm </a>
</div>
<p>I am confused now.</p>
<p>In the following lecture at 42:04 Chris manning was explaining that each word has two vector representations. One for central word and other for context word.</p>
<p><a rel="nofollow" href="https://www.youtube.com/watch?v=ERibwqs9p38&amp;index=2&amp;list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6">https://www.youtube.com/watch?v=ERibwqs9p38&amp;index=2&amp;list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6</a></p>
<p>and stack overflow is giving some explainations <a rel="nofollow" href="https://stackoverflow.com/questions/29381505/why-does-word2vec-use-2-representations-for-each-word">https://stackoverflow.com/questions/29381505/why-does-word2vec-use-2-representations-for-each-word</a> .</p>
<p>Please correct me if I am wrong. Thanks.</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130173" data-commentid="130173" data-postid="36027" data-belowelement="div-comment-130173" data-respondelement="respond" data-replyto="Reply to Ravi Theja" aria-label="Reply to Ravi Theja">Reply</a></div>
</div>
<ul class="children">
<li class="comment byuser comment-author-nss bypostauthor odd alt depth-4 parent" id="comment-130178">
<div id="div-comment-130178" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/7a65509c42ede4442b3c88e7d10cd924(1).jpeg" srcset="https://secure.gravatar.com/avatar/7a65509c42ede4442b3c88e7d10cd924?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">NSS</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130178">
June 10, 2017 at 12:50 am </a>
</div>
<p>I went through the video and the stack exchange link that you forwarded. Let us just focus on the formula manning was describing in the video exp(u.v), where V is the center word and u is the context. Now I want you to look at the skip gram excel sheet that I included in my article. You will notice that the context vector u is obtained from the weight matrix between the hidden and the output layer while the center one is given by the rows of the weight matrix between the input layer and the hidden layer. So, effectively as word is being represented by two vectors or you can interpret it as a word (one hot encoded one) projected into two different vectors. This is just a representational purpose. What I was talking in my previous comment was that, when you pass an input into the neural network, you create one vector representation(one hot encoded one) and that can be used as both the center word and the context word. Hope that, I made things clear now.</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130178" data-commentid="130178" data-postid="36027" data-belowelement="div-comment-130178" data-respondelement="respond" data-replyto="Reply to NSS" aria-label="Reply to NSS">Reply</a></div>
</div>
<ul class="children">
<li class="comment even depth-5" id="comment-130738">
<div id="div-comment-130738" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/90a90c17bcd80ebf62e6ff586f48053b.png" srcset="https://secure.gravatar.com/avatar/90a90c17bcd80ebf62e6ff586f48053b?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn"><a href="http://www.datafuture.wordpress.com/" rel="external nofollow ugc" class="url">Ravi Theja</a></cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130738">
June 19, 2017 at 2:25 pm </a>
</div>
<p>Yes, things are clear now. Thank you.</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130738" data-commentid="130738" data-postid="36027" data-belowelement="div-comment-130738" data-respondelement="respond" data-replyto="Reply to Ravi Theja" aria-label="Reply to Ravi Theja">Reply</a></div>
</div>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="comment odd alt thread-odd thread-alt depth-1 parent" id="comment-130020">
<div id="div-comment-130020" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/c09fb4441e43b86af627d34eb89c9942.png" srcset="https://secure.gravatar.com/avatar/c09fb4441e43b86af627d34eb89c9942?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">Pallavi</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130020">
June 8, 2017 at 3:30 pm </a>
</div>
<p>Thanks for this nice article! I have been waiting for word embedding related detailed article since long.</p>
<p>However, I have a query. In your excel calculations for skip gram, is the matrix multiplication shown for hidden activation row and each column from hidden output weight matrix? If yes, I am getting some different answer for output matrix. Can you please check? Or am I missing something?</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130020" data-commentid="130020" data-postid="36027" data-belowelement="div-comment-130020" data-respondelement="respond" data-replyto="Reply to Pallavi" aria-label="Reply to Pallavi">Reply</a></div>
</div>
<ul class="children">
<li class="comment byuser comment-author-nss bypostauthor even depth-2" id="comment-130111">
<div id="div-comment-130111" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/7a65509c42ede4442b3c88e7d10cd924(1).jpeg" srcset="https://secure.gravatar.com/avatar/7a65509c42ede4442b3c88e7d10cd924?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">NSS</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130111">
June 9, 2017 at 2:30 pm </a>
</div>
<p>Yes, the calculation shown is between hidden activation and hidden-output matrix. You can paste your output vector below and I can have a look.</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130111" data-commentid="130111" data-postid="36027" data-belowelement="div-comment-130111" data-respondelement="respond" data-replyto="Reply to NSS" aria-label="Reply to NSS">Reply</a></div>
</div>
</li>
</ul>
</li>
<li class="comment odd alt thread-even depth-1" id="comment-130024">
<div id="div-comment-130024" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/c09fb4441e43b86af627d34eb89c9942.png" srcset="https://secure.gravatar.com/avatar/c09fb4441e43b86af627d34eb89c9942?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">Pallavi</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130024">
June 8, 2017 at 4:01 pm </a>
</div>
<p>Thanks for the nice article!</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130024" data-commentid="130024" data-postid="36027" data-belowelement="div-comment-130024" data-respondelement="respond" data-replyto="Reply to Pallavi" aria-label="Reply to Pallavi">Reply</a></div>
</div>
</li>
<li class="comment even thread-odd thread-alt depth-1 parent" id="comment-130064">
<div id="div-comment-130064" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/cf01dcf579c7da0d901dbca62827fb9e.png" srcset="https://secure.gravatar.com/avatar/cf01dcf579c7da0d901dbca62827fb9e?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">Wouter Deketelaere</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130064">
June 9, 2017 at 1:13 am </a>
</div>
<p>Great explanation.</p>
<p>I do have a question about the co-occurrence matrix, though.</p>
<p>Shouldnât the red box contain the number 5 for a context size of 2 (around)?<br>
Around the word âintelligentâ you have two instances of âHe isâ on the left and the right of âintelligentâ.<br>
You only count the âisâ on the left and âHeâ on the right.<br>
Why is that?</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130064" data-commentid="130064" data-postid="36027" data-belowelement="div-comment-130064" data-respondelement="respond" data-replyto="Reply to Wouter Deketelaere" aria-label="Reply to Wouter Deketelaere">Reply</a></div>
</div>
<ul class="children">
<li class="comment byuser comment-author-nss bypostauthor odd alt depth-2" id="comment-130110">
<div id="div-comment-130110" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/7a65509c42ede4442b3c88e7d10cd924(1).jpeg" srcset="https://secure.gravatar.com/avatar/7a65509c42ede4442b3c88e7d10cd924?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">NSS</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130110">
June 9, 2017 at 2:26 pm </a>
</div>
<p>Read the co-occurrence matrix for the red box as â For the word âHeâ, how many times has âisâ appeared in the 2-context window. The word âintelligentâ has nothing to do with the co-occurrence of âheâ and âisâ.</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130110" data-commentid="130110" data-postid="36027" data-belowelement="div-comment-130110" data-respondelement="respond" data-replyto="Reply to NSS" aria-label="Reply to NSS">Reply</a></div>
</div>
</li>
</ul>
</li>
<li class="comment even thread-even depth-1 parent" id="comment-130560">
<div id="div-comment-130560" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/58f282aabb66861bf85445f628135d45.png" srcset="https://secure.gravatar.com/avatar/58f282aabb66861bf85445f628135d45?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">Cory</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130560">
June 16, 2017 at 2:15 am </a>
</div>
<p>Great job! Very good flow and well explained. I do have one question though regarding the output layer. For your case, you should have two vectors representing the context words we wish to âpredictâ once training is complete. When performing the multiplication between the input hidden weight matrix and hidden output weight matrix, your entries for each output vector are identical. This makes sense to me since you are using the same hidden output weight matrix and same input hidden matrix value for each of the output layers. So, after applying the softmax function, the vectors should still be identical right? A post from SO(last post of the question: <a rel="nofollow" href="https://stats.stackexchange.com/questions/194011/how-does-word2vecs-skip-gram-model-generate-the-output-vectors">https://stats.stackexchange.com/questions/194011/how-does-word2vecs-skip-gram-model-generate-the-output-vectors</a>) states that all C distributions are different, because of the softmax function. This doesnât make sense to me since given the format of softmax, each element inside of one of the C output layers would just be (for the case of the first element): element(1)/(element1+..element10). Does the change occur after the first error propogation? </p>
<p>A lecture from Stanford also displays the output layer (before softmax) to NOT be identical for each of the C windows(<a rel="nofollow" href="https://www.youtube.com/watch?v=ERibwqs9p38">https://www.youtube.com/watch?v=ERibwqs9p38</a>, time=39.22). Iâm very confused on this as Iâve had many conflicting opinions.<br>
Thanks so much for your help!</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130560" data-commentid="130560" data-postid="36027" data-belowelement="div-comment-130560" data-respondelement="respond" data-replyto="Reply to Cory" aria-label="Reply to Cory">Reply</a></div>
</div>
<ul class="children">
<li class="comment byuser comment-author-nss bypostauthor odd alt depth-2" id="comment-130728">
<div id="div-comment-130728" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/7a65509c42ede4442b3c88e7d10cd924(1).jpeg" srcset="https://secure.gravatar.com/avatar/7a65509c42ede4442b3c88e7d10cd924?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">NSS</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130728">
June 19, 2017 at 11:30 am </a>
 </div>
<p>Yes, the change occurs after the first error propagation.</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130728" data-commentid="130728" data-postid="36027" data-belowelement="div-comment-130728" data-respondelement="respond" data-replyto="Reply to NSS" aria-label="Reply to NSS">Reply</a></div>
</div>
</li>
</ul>
</li>
<li class="comment even thread-odd thread-alt depth-1" id="comment-130863">
<div id="div-comment-130863" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/ce4a563694f0db90e45ef898601b0311.png" srcset="https://secure.gravatar.com/avatar/ce4a563694f0db90e45ef898601b0311?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn"><a href="http://www.iperidigi.com/" rel="external nofollow ugc" class="url">selvin</a></cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130863">
June 21, 2017 at 11:28 am </a>
</div>
<p>very very amazing explaintionâ¦.many things gather about yourselfâ¦yes realy i enjoy it</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130863" data-commentid="130863" data-postid="36027" data-belowelement="div-comment-130863" data-respondelement="respond" data-replyto="Reply to selvin" aria-label="Reply to selvin">Reply</a></div>
</div>
</li>
<li class="comment odd alt thread-even depth-1 parent" id="comment-130961">
<div id="div-comment-130961" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/4f1e02f485e3858523f28269dd4cb498.png" srcset="https://secure.gravatar.com/avatar/4f1e02f485e3858523f28269dd4cb498?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">James Wong</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130961">
June 23, 2017 at 7:48 am </a>
</div>
<p>Hi NSS. Really great tutorial with respect to word embeddings, the best Iâve seen by far. However thereâs still a question baffling me all the time. In the CBOW algorithm, you point out that âThe weight between the hidden layer and the output layer is taken as the word vector representation of the wordâ. But under the Skip-Gram section, you then say, âThe weights between the input and the hidden layer are taken as the word vector representation after trainingâ. Could you please explain a little bit about whatâs the difference between the two weight matrices when it comes to word embedding representations?</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130961" data-commentid="130961" data-postid="36027" data-belowelement="div-comment-130961" data-respondelement="respond" data-replyto="Reply to James Wong" aria-label="Reply to James Wong">Reply</a></div>
</div>
<ul class="children">
<li class="comment byuser comment-author-nss bypostauthor even depth-2" id="comment-131033">
<div id="div-comment-131033" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/7a65509c42ede4442b3c88e7d10cd924(1).jpeg" srcset="https://secure.gravatar.com/avatar/7a65509c42ede4442b3c88e7d10cd924?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">NSS</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-131033">
June 24, 2017 at 10:59 am </a>
</div>
<p>It is more of an empirical choice. You can choose either weights but generally what people use is the weight matrix near to the single word as vector representation. For example in CBOW, the output is a single word so weight matrix between hidden and output is preferred while in skip gram, input word is a single word, so the weight matrix between input and hidden is preferred.</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-131033" data-commentid="131033" data-postid="36027" data-belowelement="div-comment-131033" data-respondelement="respond" data-replyto="Reply to NSS" aria-label="Reply to NSS">Reply</a></div>
</div>
</li>
</ul>
</li>
<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-138901">
<div id="div-comment-138901" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/e533118d5e8bd9c25a15165ce52bd774.png" srcset="https://secure.gravatar.com/avatar/e533118d5e8bd9c25a15165ce52bd774?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">Pizza Boy</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-138901">
October 6, 2017 at 8:35 pm </a>
</div>
<p>Very good explanation about word2vecs. Also illustration is wonderful. However, I want to learn how you can acquire this knowledge. <img draggable="false" role="img" class="emoji" alt="ð" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/1f642.svg"> Please make more content like this.</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-138901" data-commentid="138901" data-postid="36027" data-belowelement="div-comment-138901" data-respondelement="respond" data-replyto="Reply to Pizza Boy" aria-label="Reply to Pizza Boy">Reply</a></div>
</div>
</li>
<li class="comment even thread-even depth-1" id="comment-139194">
<div id="div-comment-139194" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/5aaaceb6bf3766d3d1f4ea30391d7729.jpeg" srcset="https://secure.gravatar.com/avatar/5aaaceb6bf3766d3d1f4ea30391d7729?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn"><a href="http://www.fanyeong.com/" rel="external nofollow ugc" class="url">Yongyong Fan</a></cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-139194">
October 10, 2017 at 9:30 am </a>
</div>
<p>Hey bro, thatâs really an awesome article, very well explained!!! The original paper is hard to understand for NLP beginners. Thank you!!!</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-139194" data-commentid="139194" data-postid="36027" data-belowelement="div-comment-139194" data-respondelement="respond" data-replyto="Reply to Yongyong Fan" aria-label="Reply to Yongyong Fan">Reply</a></div>
</div>
</li>
<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-140408">
<div id="div-comment-140408" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/3345f21321a61f3cbfc01287c299c16a.png" srcset="https://secure.gravatar.com/avatar/3345f21321a61f3cbfc01287c299c16a?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">Tansu</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-140408">
October 22, 2017 at 3:36 pm </a>
</div>
<p>Excellent resource. Clear and very well organized. I was trying to understand the concepts for two days and this is the best one. Thanks.</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-140408" data-commentid="140408" data-postid="36027" data-belowelement="div-comment-140408" data-respondelement="respond" data-replyto="Reply to Tansu" aria-label="Reply to Tansu">Reply</a></div>
</div>
</li>
<li class="comment even thread-even depth-1" id="comment-140703">
<div id="div-comment-140703" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/00d80f937b1463b90ed461851dfacf93.png" srcset="https://secure.gravatar.com/avatar/00d80f937b1463b90ed461851dfacf93?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">sharath chandra</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-140703">
October 25, 2017 at 2:18 am </a>
</div>
<p>Thanks for the great intuition tutorial . I have one question regarding context window.</p>
<p>For the sample corpus C = âHey, this is sample corpus using only one context word.â in CBOW and Skip-Gram models, where context window is 1, the input-outputs you have shown are [hey, this],[this, hey], [is, this], [is, sample] but why we skipped [this, is]</p>
<p>Is it for any specific reason. Please help me if Iâm understanding the context window wrongly.</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-140703" data-commentid="140703" data-postid="36027" data-belowelement="div-comment-140703" data-respondelement="respond" data-replyto="Reply to sharath chandra" aria-label="Reply to sharath chandra">Reply</a></div>
</div>
</li>
<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-141942">
<div id="div-comment-141942" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/f99fcf352479af0d2173cc8d3c182766.png" srcset="https://secure.gravatar.com/avatar/f99fcf352479af0d2173cc8d3c182766?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn"><a href="http://www.bidiwe.com/" rel="external nofollow ugc" class="url">Fofie</a></cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-141942">
November 2, 2017 at 8:06 pm </a>
</div>
<p>Very nice article thanks a lotâ¦..</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-141942" data-commentid="141942" data-postid="36027" data-belowelement="div-comment-141942" data-respondelement="respond" data-replyto="Reply to Fofie" aria-label="Reply to Fofie">Reply</a></div>
</div>
</li>
<li class="comment even thread-even depth-1" id="comment-142173">
<div id="div-comment-142173" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/f5dd16e9759ef2aa4e5dfac4d1bb2d96.png" srcset="https://secure.gravatar.com/avatar/f5dd16e9759ef2aa4e5dfac4d1bb2d96?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">Veena</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-142173">
November 4, 2017 at 1:07 pm </a>
</div>
<p>Very good article.</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-142173" data-commentid="142173" data-postid="36027" data-belowelement="div-comment-142173" data-respondelement="respond" data-replyto="Reply to Veena" aria-label="Reply to Veena">Reply</a></div>
</div>
</li>
<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-143663">
<div id="div-comment-143663" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/fd3ce3d0fb8d2ce53cead61bb75eb62e.png" srcset="https://secure.gravatar.com/avatar/fd3ce3d0fb8d2ce53cead61bb75eb62e?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">Shahbaz Hassan Wasti</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-143663">
November 14, 2017 at 11:44 am </a>
</div>
<p>Thanks man for this great contribution, really by far the best tutorial to lean Word2Vec and related concepts. The amazing thing about your explanation is that you have provided a comprehensive understanding of the concepts yet in a simplest possible way.</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-143663" data-commentid="143663" data-postid="36027" data-belowelement="div-comment-143663" data-respondelement="respond" data-replyto="Reply to Shahbaz Hassan Wasti" aria-label="Reply to Shahbaz Hassan Wasti">Reply</a></div>
</div>
</li>
<li class="comment even thread-even depth-1" id="comment-151522">
<div id="div-comment-151522" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/f54a0886ced044630433bbbf1a8fc534.jpeg" srcset="https://secure.gravatar.com/avatar/f54a0886ced044630433bbbf1a8fc534?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">Ali</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-151522">
February 21, 2018 at 5:38 pm </a>
</div>
<p>That is a really great article !<br>
things are simple yet powerful and clear</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-151522" data-commentid="151522" data-postid="36027" data-belowelement="div-comment-151522" data-respondelement="respond" data-replyto="Reply to Ali" aria-label="Reply to Ali">Reply</a></div>
</div>
</li>
<li class="comment odd alt thread-odd thread-alt depth-1 parent" id="comment-152072">
<div id="div-comment-152072" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/68910bfe21dacb4d2c53af31cb38aab0.png" srcset="https://secure.gravatar.com/avatar/68910bfe21dacb4d2c53af31cb38aab0?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">Saul Goodman</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-152072">
March 22, 2018 at 12:12 am </a>
</div>
<p>Thank you. Very minor edit </p>
<p>In CBOW example this, is is missing as the third data point.</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-152072" data-commentid="152072" data-postid="36027" data-belowelement="div-comment-152072" data-respondelement="respond" data-replyto="Reply to Saul Goodman" aria-label="Reply to Saul Goodman">Reply</a></div>
</div>
<ul class="children">
<li class="comment byuser comment-author-jalfaizy even depth-2" id="comment-152085">
<div id="div-comment-152085" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/fd90df6aafd127cc243609d0e1a97621.jpeg" srcset="https://secure.gravatar.com/avatar/fd90df6aafd127cc243609d0e1a97621?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">Faizan Shaikh</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-152085">
March 22, 2018 at 6:00 pm </a>
</div>
<p>Thanks Saul for the feedback. I have updated the article</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-152085" data-commentid="152085" data-postid="36027" data-belowelement="div-comment-152085" data-respondelement="respond" data-replyto="Reply to Faizan Shaikh" aria-label="Reply to Faizan Shaikh">Reply</a></div>
</div>
</li>
</ul>
</li>
<li class="comment odd alt thread-even depth-1" id="comment-153393">
<div id="div-comment-153393" class="comment-body">
<div class="comment-author vcard">
<img alt="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/926deee3af9f5d20b508f28dd258cde2.png" srcset="https://secure.gravatar.com/avatar/926deee3af9f5d20b508f28dd258cde2?s=200&amp;d=mm&amp;r=g 2x" class="avatar avatar-100 photo" height="100" width="100" loading="lazy"> <cite class="fn">Aakash Dubey</cite> <span class="says">says:</span> </div>
<div class="comment-meta commentmetadata"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-153393">
May 19, 2018 at 1:30 pm </a>
</div>
<p>Hey NSS,</p>
<p>Very Useful Article Bro <img draggable="false" role="img" class="emoji" alt="ð" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/1f642.svg"> Thank You For Sharing It</p>
<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-153393" data-commentid="153393" data-postid="36027" data-belowelement="div-comment-153393" data-respondelement="respond" data-replyto="Reply to Aakash Dubey" aria-label="Reply to Aakash Dubey">Reply</a></div>
</div>
</li>
</ul>
</div>
<div id="write-comment"></div>
<script>
var hash = window.location.hash;
if(hash!=""){
jQuery.ajax ({
	type: 'GET',
    url: 'https://www.analyticsvidhya.com/comments-form/show-comments.php?rand='+Math.random()+'&id=36027',
    success: function (data)
	{
		jQuery('#show-comments').html(data);
	},
	error: function (e) {
		console.log(e);
	}
});
}
</script>
</div>
</div>
<div class="col-sm-4 "><div class="sidebar q2w3-fixed-widget-container" style="height: 2213.67px;"><div id="text-24" class="widget clearfix widget_text"> <div class="textwidget"><div id="time-series-disp"><a href="https://courses.analyticsvidhya.com/courses/data-science-hacks-tips-and-tricks?utm_source=hacksandtipsbanner&amp;utm_medium=blog" target="_blank"><img src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/free-ds.webp"></a></div></div>
</div><div id="text-44" class="widget clearfix widget_text"> <div class="textwidget"><p><script async="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/f.txt"></script><br>
<br>
<ins class="adsbygoogle" style="display: inline-block; width: 336px; height: 280px;" data-ad-client="ca-pub-5229672700622157" data-ad-slot="1620249624"></ins><br>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></p>
</div>
</div><div id="wmp_widget-2" class="widget clearfix widget_wmp_widget"><div class="widget-title"><div class="block-cap"><h3>Popular posts</h3></div></div><ul class="wp-most-popular">
<li class="post-35058posttype-poststatus-publishformat-standardhas-post-thumbnailcategory-careercategory-intermediatecategory-interviewscategory-machine-learningcategory-skilltesttag-data-science-interviewstag-essential-machine-learning-skillstag-machine-learningtag-machine-learning-algorithmtag-machine-learning-application">
<a href="https://www.analyticsvidhya.com/blog/2017/04/40-questions-test-data-scientist-machine-learning-solution-skillpower-machine-learning-datafest-2017/" title="40 Questions to test a data scientist on Machine Learning [Solution: SkillPower â Machine Learning, DataFest 2017]">
40 Questions to test a data scientist on Machine Learning [Solution: SkillPower â Machine Learning, DataFest 2017]
</a>
</li>
<li class="post-70584posttype-poststatus-publishformat-standardhas-post-thumbnailcategory-beginnercategory-business-analyticscategory-excelcategory-linear-regressiontag-forecastingtag-linear-regressiontag-time-series">
<a href="https://www.analyticsvidhya.com/blog/2020/09/how-to-build-forecast-excel/" title="How to Build a Sales Forecast using Microsoft Excel in Just 10 Minutes!">
How to Build a Sales Forecast using Microsoft Excel in Just 10 Minutes!
</a>
</li>
<li class="post-18770posttype-poststatus-publishformat-standardhas-post-thumbnailcategory-algorithmcategory-business-analyticscategory-classificationcategory-clusteringcategory-intermediatecategory-listiclecategory-machine-learningcategory-python-2category-rcategory-regressioncategory-structured-datacategory-supervisedcategory-unsupervisedtag-c4-5tag-carttag-catboosttag-data-sciencetag-decision-treetag-gbmtag-k-meanstag-knntag-lightgbmtag-linear-regressiontag-live-codingtag-logistic-regressiontag-machine-learningtag-naive-bayestag-neural-networktag-random-foresttag-reinforcementtag-supervised-learningtag-svmtag-unsupervisedtag-xgboost">
<a href="https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/" title="Commonly used Machine Learning Algorithms (with Python and R Codes)">
Commonly used Machine Learning Algorithms (with Python and R Codes)
</a>
</li>
<li class="post-33160posttype-poststatus-publishformat-standardhas-post-thumbnailcategory-beginnercategory-business-analyticscategory-mathscategory-rcategory-techniquetag-excel-solvertag-least-cost-methodtag-linear-optimizationtag-linear-programmtag-linear-programmingtag-linear-programming-for-data-sciencetag-linear-programming-modelstag-northwest-corner-methodtag-opensolvertag-simplex-method">
<a href="https://www.analyticsvidhya.com/blog/2017/02/lintroductory-guide-on-linear-programming-explained-in-simple-english/" title="Introductory guide on Linear Programming for (aspiring) data scientists">
Introductory guide on Linear Programming for (aspiring) data scientists
</a>
</li>
<li class="post-32410posttype-poststatus-publishformat-standardcategory-careercategory-deep-learningcategory-intermediatecategory-machine-learningcategory-skilltesttag-deep-learningtag-deep-learning-applicationtag-deep-learning-coursestag-deep-learning-guidetag-deep-learning-tutorialstag-gpu-for-deep-learningtag-guide-on-deep-learningtag-implementation-of-neural-networkstag-neural-networktag-neural-network-coursestag-neural-network-tutorials">
<a href="https://www.analyticsvidhya.com/blog/2017/01/must-know-questions-deep-learning/" title="45 Questions to test a data scientist on basics of Deep Learning (along with solution)">
45 Questions to test a data scientist on basics of Deep Learning (along with solution)
</a>
</li>
<li class="post-32906posttype-poststatus-publishformat-standardhas-post-thumbnailcategory-business-analyticscategory-careercategory-intermediatecategory-machine-learningcategory-rcategory-skilltesttag-clusteringtag-clustering-analysistag-clustering-skilltest-solutiontag-clustering-techniquestag-hierarchical-clusteringtag-k-means-clusteringtag-non-hierarchical-clusteringtag-portfolio-clusteringtag-two-step-clustering">
<a href="https://www.analyticsvidhya.com/blog/2017/02/test-data-scientist-clustering/" title="40 Questions to test a Data Scientist on Clustering Techniques (Skill test Solution)">
40 Questions to test a Data Scientist on Clustering Techniques (Skill test Solution)
</a>
</li>
<li class="post-36659posttype-poststatus-publishformat-standardhas-post-thumbnailcategory-advancedcategory-careercategory-educationcategory-linear-regressioncategory-machine-learningcategory-skilltesttag-lasso-regressiontag-linear-regressiontag-machine-learningtag-regressiontag-ridge-regression">
<a href="https://www.analyticsvidhya.com/blog/2017/07/30-questions-to-test-a-data-scientist-on-linear-regression/" title="30 Questions to test a data scientist on Linear Regression [Solution: Skilltest â Linear Regression]">
30 Questions to test a data scientist on Linear Regression [Solution: Skilltest â Linear Regression]
</a>
</li>
<li class="post-19875posttype-poststatus-publishformat-standardhas-post-thumbnailcategory-beginnercategory-data-sciencecategory-machine-learningcategory-mathscategory-probabilitycategory-python-2category-rcategory-techniquetag-bayes-theoremtag-classificationtag-conditional-probabilitytag-data-sciencetag-live-codingtag-machine-learningtag-naive-bayestag-naive-bayes-classifiertag-naive-bayes-in-rtag-probabilitytag-python">
<a href="https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/" title="6 Easy Steps to Learn Naive Bayes Algorithm with codes in Python and R">
6 Easy Steps to Learn Naive Bayes Algorithm with codes in Python and R
</a>
</li>
</ul></div><div id="text-23" class="widget clearfix widget_text"> <div class="textwidget"><script async="" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/f.txt"></script>

<ins class="adsbygoogle" style="display:inline-block;width:336px;height:280px" data-ad-client="ca-pub-5229672700622157" data-ad-slot="1620249624"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
</div><div id="maha_recent_posts-3" class="widget clearfix widget_recents"><div class="widget-title"><div class="block-cap"><h3>Recent Posts</h3></div></div>
<article class="post-box-small recent-item clearfix">
<div class="thumb-wrap zoom-zoom three">
<a href="https://www.analyticsvidhya.com/blog/2020/10/how-does-the-gradient-descent-algorithm-work-in-machine-learning/" rel="bookmark" title="How Does the Gradient Descent Algorithm Work in Machine Learning?">
<img width="83" height="63" class="entry-thumb zoom-it three" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/Machine-Learning-Libraries-C-83x63.jpg" alt="How Does the Gradient Descent Algorithm Work in Machine Learning?" title="How Does the Gradient Descent Algorithm Work in Machine Learning?">
</a>
</div>
<div class="box-small-wrap">
<h3 class="entry-title short-bottom">
<a href="https://www.analyticsvidhya.com/blog/2020/10/how-does-the-gradient-descent-algorithm-work-in-machine-learning/" rel="bookmark" title="How Does the Gradient Descent Algorithm Work in Machine Learning?">
How Does the Gradient Descent Algorithm Work in Machine Learning? </a>
</h3>
<meta content="How Does the Gradient Descent Algorithm Work in Machine Learning?">
<div class="meta-info">
<time class="entry-date" datetime="2020-10-02T15:47:48+05:30">
October 2, 2020 </time>
</div>
</div>
</article>
<article class="post-box-small recent-item clearfix">
<div class="thumb-wrap zoom-zoom three">
<a href="https://www.analyticsvidhya.com/blog/2020/10/cost-complexity-pruning-decision-trees/" rel="bookmark" title="Letâs Solve Overfitting! Quick Guide to Cost Complexity Pruning of Decision Trees">
<img width="83" height="63" class="entry-thumb zoom-it three" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/featured-image-83x63.jpg" alt="Letâs Solve Overfitting! Quick Guide to Cost Complexity Pruning of Decision Trees" title="Letâs Solve Overfitting! Quick Guide to Cost Complexity Pruning of Decision Trees">
</a>
</div>
<div class="box-small-wrap">
<h3 class="entry-title short-bottom">
<a href="https://www.analyticsvidhya.com/blog/2020/10/cost-complexity-pruning-decision-trees/" rel="bookmark" title="Letâs Solve Overfitting! Quick Guide to Cost Complexity Pruning of Decision Trees">
Letâs Solve Overfitting! Quick Guide to Cost Complexity Pruning of Decision Trees </a>
</h3>
<meta content="Letâs Solve Overfitting! Quick Guide to Cost Complexity Pruning of Decision Trees">
<div class="meta-info">
<time class="entry-date" datetime="2020-10-02T14:46:04+05:30">
October 2, 2020 </time>
</div>
</div>
</article>
<article class="post-box-small recent-item clearfix">
<div class="thumb-wrap zoom-zoom three">
<a href="https://www.analyticsvidhya.com/blog/2020/10/introduction-python-functions-data-science/" rel="bookmark" title="Introduction to Python Functions for Data Science Beginners">
<img width="83" height="63" class="entry-thumb zoom-it three" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/Dealing-with-Datetime-in-Python.webp" alt="Introduction to Python Functions for Data Science Beginners" title="Introduction to Python Functions for Data Science Beginners">
</a>
</div>
<div class="box-small-wrap">
<h3 class="entry-title short-bottom">
<a href="https://www.analyticsvidhya.com/blog/2020/10/introduction-python-functions-data-science/" rel="bookmark" title="Introduction to Python Functions for Data Science Beginners">
Introduction to Python Functions for Data Science Beginners </a>
</h3>
<meta content="Introduction to Python Functions for Data Science Beginners">
<div class="meta-info">
<time class="entry-date" datetime="2020-10-01T20:13:37+05:30">
October 1, 2020 </time>
</div>
</div>
</article>
<article class="post-box-small recent-item clearfix">
<div class="thumb-wrap zoom-zoom three">
<a href="https://www.analyticsvidhya.com/blog/2020/10/hacklive-data-science-hackathons/" rel="bookmark" title="HackLive â Everything You Need to Get Started with Data Science Hackathons!">
<img width="83" height="63" class="entry-thumb zoom-it three" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/Master-the-art-of-participating-in-Data-science-competitions-HackLive-83x63.jpg" alt="HackLive â Everything You Need to Get Started with Data Science Hackathons!" title="HackLive â Everything You Need to Get Started with Data Science Hackathons!">
</a>
</div>
<div class="box-small-wrap">
<h3 class="entry-title short-bottom">
<a href="https://www.analyticsvidhya.com/blog/2020/10/hacklive-data-science-hackathons/" rel="bookmark" title="HackLive â Everything You Need to Get Started with Data Science Hackathons!">
HackLive â Everything You Need to Get Started with Data Science Hackathons! </a>
</h3>
<meta content="HackLive â Everything You Need to Get Started with Data Science Hackathons!">
<div class="meta-info">
<time class="entry-date" datetime="2020-10-01T15:49:19+05:30">
October 1, 2020 </time>
</div>
</div>
</article>
</div><div id="text-42_clone" class="widget clearfix widget_text q2w3-widget-clone-blog-sidebar" style="top: 55px; width: 304.99px; height: 257px; visibility: hidden;"> 
</div><div id="text-42" class="widget clearfix widget_text" style="top: 55px; width: 304.99px; position: fixed;"> <div class="textwidget"><p><a href="https://courses.analyticsvidhya.com/pages/all-free-courses/?utm_source=Sticky_banner1&amp;utm_medium=display&amp;utm_campaign=Free_Certified_Courses_September" target="_blank" rel="noopener noreferrer"><img src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/ds-dl-ml-1.webp"></a></p>
</div>
</div><div id="text-45_clone" class="widget clearfix widget_text q2w3-widget-clone-blog-sidebar" style="top: 344px; width: 304.99px; height: 361px; visibility: hidden;"> 
</div><div id="text-45" class="widget clearfix widget_text" style="top: 344px; width: 304.99px; position: fixed;"><div class="widget-title"><div class="block-cap"><h3>Jobs &amp; Internships</h3></div></div> <div class="textwidget"><div id="jobs-show">

<div class="api-card">
  <ul>
    
    <li>
      <div class="api-card-content">
        <meta content="Data Science Intern">
        <a href="https://jobsnew.analyticsvidhya.com/jobs/data-science-intern-2" title="Data Science Intern">
          <h3>Data Science Intern</h3>
        </a>
        <p>Analytics Vidhya | <i><img src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/location.svg" alt="location"></i> Gurgaon / Remote </p>
      </div>
      <a href="https://jobsnew.analyticsvidhya.com/jobs/data-science-intern-2" target="_blank" class="apply-now">Apply Now</a>

    </li>
    
    <li>
      <div class="api-card-content">
        <meta content="Senior AI Developer">
        <a href="https://jobsnew.analyticsvidhya.com/jobs/chief-technology-officer" title="Senior AI Developer">
          <h3>Senior AI Developer</h3>
        </a>
        <p>Stupa Sports Analytics | <i><img src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/location.svg" alt="location"></i> New Delhi </p>
      </div>
      <a href="https://jobsnew.analyticsvidhya.com/jobs/chief-technology-officer" target="_blank" class="apply-now">Apply Now</a>

    </li>
    
    <li>
      <div class="api-card-content">
        <meta content="ML Engineer">
        <a href="https://jobsnew.analyticsvidhya.com/jobs/ml-engineer" title="ML Engineer">
          <h3>ML Engineer</h3>
        </a>
        <p>Precily R&amp;D Private Limited | <i><img src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/location.svg" alt="location"></i> New Delhi </p>
      </div>
      <a href="https://jobsnew.analyticsvidhya.com/jobs/ml-engineer" target="_blank" class="apply-now">Apply Now</a>

    </li>
    
    <li>
      <div class="api-card-content">
        <meta content="Data Science Intern - Analytics Vidhya - Gurgaon">
        <a href="https://jobsnew.analyticsvidhya.com/jobs/data-science-intern-analytics-vidhya-gurgaon" title="Data Science Intern - Analytics Vidhya - Gurgaon">
          <h3>Data Science Intern - Analytics Vidhya - Gurgaon</h3>
        </a>
        <p>Analytics Vidhya | <i><img src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/location.svg" alt="location"></i> Gurugram </p>
      </div>
      <a href="https://jobsnew.analyticsvidhya.com/jobs/data-science-intern-analytics-vidhya-gurgaon" target="_blank" class="apply-now">Apply Now</a>

    </li>
    
    <li>
      <div class="api-card-content">
        <meta content="Data Science Intern - Analytics Vidhya - Gurugram (6 Months)">
        <a href="https://jobsnew.analyticsvidhya.com/jobs/data-science-intern-analytics-vidhya-gurugram-6-months" title="Data Science Intern - Analytics Vidhya - Gurugram (6 Months)">
          <h3>Data Science Intern - Analytics Vidhya - Gurugram (6 Months)</h3>
        </a>
        <p>A Client of Analytics Vidhya | <i><img src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/location.svg" alt="location"></i> Multiple </p>
      </div>
      <a href="https://jobsnew.analyticsvidhya.com/jobs/data-science-intern-analytics-vidhya-gurugram-6-months" target="_blank" class="apply-now">Apply Now</a>

    </li>
    
    <li>
      <div class="api-card-content">
        <meta content="Data Scientist - Delhi/NCR (0-2 Years Of Experience)">
        <a href="https://jobsnew.analyticsvidhya.com/jobs/data-scientist-delhincr-0-2-years-of-experience" title="Data Scientist - Delhi/NCR (0-2 Years Of Experience)">
          <h3>Data Scientist - Delhi/NCR (0-2 Years Of Experience)</h3>
        </a>
        <p>A Client of Analytics Vidhya | <i><img src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/location.svg" alt="location"></i> Delhi </p>
      </div>
      <a href="https://jobsnew.analyticsvidhya.com/jobs/data-scientist-delhincr-0-2-years-of-experience" target="_blank" class="apply-now">Apply Now</a>

    </li>
    
    <li>
      <div class="api-card-content">
        <meta content="DL Internship- Chennai (3 Months)">
        <a href="https://jobsnew.analyticsvidhya.com/jobs/dl-internship-chennai-3-months" title="DL Internship- Chennai (3 Months)">
          <h3>DL Internship- Chennai (3 Months)</h3>
        </a>
        <p>A Client of Analytics Vidhya | <i><img src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/location.svg" alt="location"></i> Chennai </p>
      </div>
      <a href="https://jobsnew.analyticsvidhya.com/jobs/dl-internship-chennai-3-months" target="_blank" class="apply-now">Apply Now</a>

    </li>
    
    <li>
      <div class="api-card-content">
        <meta content="Intern- Data Analytics- Gurgaon (2-6 Months)">
        <a href="https://jobsnew.analyticsvidhya.com/jobs/intern-data-analytics-gurgaon-2-6-months" title="Intern- Data Analytics- Gurgaon (2-6 Months)">
          <h3>Intern- Data Analytics- Gurgaon (2-6 Months)</h3>
        </a>
        <p>A Client of Analytics Vidhya | <i><img src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/location.svg" alt="location"></i> Gurugram </p>
      </div>
      <a href="https://jobsnew.analyticsvidhya.com/jobs/intern-data-analytics-gurgaon-2-6-months" target="_blank" class="apply-now">Apply Now</a>

    </li>
    
    <li>
      <div class="api-card-content">
        <meta content="Engagement Manager - Strategy - Technology (7-11 yrs)">
        <a href="https://jobsnew.analyticsvidhya.com/jobs/engagement-manager-strategy-technology-7-11-yrs" title="Engagement Manager - Strategy - Technology (7-11 yrs)">
          <h3>Engagement Manager - Strategy - Technology (7-11 yrs)</h3>
        </a>
        <p>A Client of Analytics Vidhya | <i><img src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/location.svg" alt="location"></i> Delhi </p>
      </div>
      <a href="https://jobsnew.analyticsvidhya.com/jobs/engagement-manager-strategy-technology-7-11-yrs" target="_blank" class="apply-now">Apply Now</a>

    </li>
    
    <li>
      <div class="api-card-content">
        <meta content="Credit Risk Analytics - Retail/SME Banking (3-15 yrs)">
        <a href="https://jobsnew.analyticsvidhya.com/jobs/credit-risk-analytics-retailsme-banking-3-15-yrs" title="Credit Risk Analytics - Retail/SME Banking (3-15 yrs)">
          <h3>Credit Risk Analytics - Retail/SME Banking (3-15 yrs)</h3>
        </a>
        <p>A Client of Analytics Vidhya | <i><img src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/location.svg" alt="location"></i> Delhi </p>
      </div>
      <a href="https://jobsnew.analyticsvidhya.com/jobs/credit-risk-analytics-retailsme-banking-3-15-yrs" target="_blank" class="apply-now">Apply Now</a>

    </li>
    
  </ul>
</div>
<style>
  .api-card {
    width: 100%;
    float: left;
    background: #fff;
    font-family: Arial, Helvetica, sans-serif;
  }

  .api-card a {
    color: #4a4a4a;
    text-decoration: none;
    display: block;
  }

  .api-card a:hover,
  .api-card a:focus {
    text-decoration: none;
  }

  .api-card ul {
    margin: 0px;
    padding: 0px;
  }

  .api-card ul li {
    width: 100%;
    list-style: none;
    margin: 0 0 10px 0;
    padding: 10px;
    border-bottom: solid 1px #dedede;
    float: left;
    clear: both;
    box-sizing: border-box;
  }

  .api-card ul li .api-card-content h3 {
    margin: 0 0 10px 0;
    padding: 0px;
    display: block;
    color: #222;
    font-size: 14px;
    font-weight: 500;
    line-height: 25px;
  }

  .api-card ul li .api-card-content p {
    color: #a5a5a5;
    margin: 0px;
    font-size: 12px;
    display: flex;
    align-items: center;
  }
  .api-card ul li .api-card-content i {
    width: 12px;
    margin: 0 3px;
  }
  .api-card ul li .api-card-content i img {
    max-width: 100%;
    margin: 0 auto;
    opacity: 0.2;
  }

  .apply-now {
    background: #007aff;
    padding: 6px 15px;
    border-radius: 30px;
    color: #fff !important;
    margin: 10px 0 0 0px;
    font-size: 12px;
    font-weight: 400;
    text-align: center;
    vertical-align: middle;
    -webkit-user-select: none;
    -moz-user-select: none;
    -ms-user-select: none;
    user-select: none;
    line-height: 1.5;
    float: right;
}

</style>
</div>
</div>
</div></div></div> </div>
</div>
</div>
</div>

</div>

<footer class="footer-bg">
<div class="container">
<div class="row">
<div class="col-lg-4 col-md-4 col-sm-4 col-12">
<a href="https://www.analyticsvidhya.com/">
<img src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/av-white2.webp" alt="Analytics Vidhya" width="85%">
</a>
<div class="download-av-app">
<h6 class="d-flex align-items-center download-text-center">Download App</h6>
<a href="https://play.google.com/store/apps/details?id=com.analyticsvidhya.android" target="_blank">
<img src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/play_store_icon.svg" alt="play store">
</a>
<a href="https://apps.apple.com/us/app/analytics-vidhya/id1470025572" target="_blank">
<img src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/app_Store_icon.svg" alt="App Store">
</a>
</div>
</div>
<div class="col-lg-8 col-md-8 col-sm-8 col-12">
<div class="av-links">
<ul>
<li>
<h6>Analytics Vidhya</h6>
<a href="https://www.analyticsvidhya.com/about-me/">About Us</a>
<a href="https://www.analyticsvidhya.com/about-me/team/">Our Team</a>
<a href="https://www.analyticsvidhya.com/about-me/career-analytics-vidhya/">Careers</a>
<a href="https://www.analyticsvidhya.com/contact/">Contact us</a>
</li>
<li>
<h6>Data Science</h6>
<a href="https://www.analyticsvidhya.com/blog/">Blog</a>
<a href="https://datahack.analyticsvidhya.com/">Hackathon</a>
<a href="https://discuss.analyticsvidhya.com/">Discussions</a>
<a href="https://www.analyticsvidhya.com/jobs/">Apply Jobs</a>
</li>
<li>
<h6>Companies</h6>
<a href="https://www.analyticsvidhya.com/corporate/">Post Jobs</a>
<a href="https://courses.analyticsvidhya.com/">Trainings</a>
<a href="https://datahack.analyticsvidhya.com/">Hiring Hackathons</a>
<a href="https://www.analyticsvidhya.com/contact/">Advertising</a>
</li>
<li>
<h6>Visit us</h6>
<div class="av-get-social">
<a href="https://www.facebook.com/AnalyticsVidhya/" target="_blank" class="social-visit fb mr-2">
<i class="fa fa-facebook"></i>
</a>
<a href="https://www.linkedin.com/company/analytics-vidhya/" target="_blank" class="social-visit in mr-2">
<i class="fa fa-linkedin"></i>
</a>
<a href="https://www.youtube.com/channel/UCH6gDteHtH4hg3o2343iObA" target="_blank" class="social-visit utub mr-2">
<i class="fa fa-youtube"></i>
</a>
<a href="https://twitter.com/analyticsvidhya" target="_blank" class="social-visit tw mr-2">
<i class="fa fa-twitter"></i>
</a>
</div>
</li>
</ul>
</div>
</div>
</div>
</div>

<div class="footer-link-bottom">
<div class="container">
<div class="row">
<div class="col-12 col-sm-6 col-md-6">
<div class="mr-auto">
<p>Â© Copyright 2013-2020 Analytics Vidhya</p>
</div>
</div>
<div class="col-12 col-sm-6 col-md-6">
<div class="footer-float-right">
<a href="https://www.analyticsvidhya.com/privacy-policy/" target="_blank">Privacy Policy</a>
<a href="https://www.analyticsvidhya.com/terms/" target="_blank">Terms of Use</a>
<a href="https://www.analyticsvidhya.com/refund-policy/" target="_blank">Refund Policy</a>
</div>
</div>
</div>
</div>
</div>

</footer>
</div>

</div>

<div id="scrolltop" class="scroll-up">
<a><i class="tm-go-top"></i></a>
</div>

<div>
<a target="_blank" href="https://www.analyticsvidhya.com/back-channel/download-pdf.php?utm_source=pdf&amp;pid=36027&amp;next=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/" title="Download PDF of this Article">
<div class="floating-btn fb-red">
<img src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/pdf.webp">
</div>
</a>
</div>
<div>
<a id="fb-blue-href" href="https://id.analyticsvidhya.com/auth/login/?next=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/" target="_blank" title="Login to Bookmark this article">
<div class="floating-btn fb-blue">
<img src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/bookmark.webp">
</div>
</a>
</div>
<script>
 $(document).ready(function(){
	     $.ajax({
        url: "https://www.analyticsvidhya.com/back-channel/display-image.php",
        data: {
            image: '3'
        },
        type: "GET",
				cache: false,
        dataType: "html",
        success: function (data) {
            $('#ban3_av').html(data);
        },
        error: function (xhr, status) {
					console.log("Image Failed!");
        },
    });
});
</script>
<script type="text/javascript" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/oidc-client.js" defer=""></script>
<script>
var redirect_uri = 'https://www.analyticsvidhya.com/';
var authority_id = 'https://id.analyticsvidhya.com/openid';
var client_o_id = '516577';
</script>
<script type="text/javascript" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/id-connect-v2.js" defer=""></script>
<script>
var img_path = "https://www.analyticsvidhya.com/wp-content/themes/Curated/";
var bookmark_post_id = "36027"
var is_bookmarked = false;
var uid = "";

function user_logged_in(user)
{
    $.ajaxSetup({
            beforeSend: function (xhr){
                xhr.setRequestHeader("Authorization","Bearer "+user.access_token);
             }
    });//ajaxsetup
    // Check if the article is already bookmarked
    $.ajax({
     type:"GET",
     url:"https://id.analyticsvidhya.com/api/v1/resources/external_save_check?resource_ids=36027",
     contentType: 'application/json',
     success: function(data) {
         if(data.results[0].is_bookmarked==true)
            {
              $(".save-for-later").html('<a id="bookmark-article" href="#"><img src="'+img_path+'/custom-design/images/bookmarked.svg" width="18px" alt="Remove Bookmark" title="Remove Bookmark" data-toggle="tooltip"></a>');
							$(".fb-blue").addClass("fb-active-blue");
							$("#fb-blue-href").attr("title","Remove Bookmark");
							is_bookmarked = true;
						}
     },
        error: function(data) {
        data =  JSON.stringify(data);
//        $(".save-for-later").html(data);
       }
    });
   // End Check if article already Bookmarked

	// If user is logged in give option to bookmark article
        $("#login1").html("<a href='https://id.analyticsvidhya.com/accounts/profile/' target='_blank'>" + user.profile.preferred_username + "</a>");
        $(".save-for-later").html('<a id="bookmark-article" href="#"><img src="'+img_path+'/custom-design/images/do-bookmark.svg" width="18px" alt="Bookmark this story to read later" title="Bookmark this story to read later" data-toggle="tooltip"></a>');
				$("#fb-blue-href").attr("title","Bookmark this story to read later");
				$(".download-pdf").html("<a href='https://www.analyticsvidhya.com/back-channel/download-pdf.php?pid=36027&next="+$(location).attr('href')+ "' ><img src='https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/download-post-pdf.png' alt='Click to Download PDF' title='Click to Download PDF of this Article'></a>");
				$("#top-av-users-signup").hide();
				$("#top-av-users-show").show();
				$("#is_logged_in_brchr").val("1");
				$(".coding-window-no-show").hide();
				initCodingWindow();
				$(".coding-window").show();
				$(".fluffycourse-hide").hide();
				$(".fluffycourse").show();
				uid = user.profile.sub;
				quizInit(uid);
				// Popup Logged in
				$("#popup_logged_in").val("1");
				//console.log ("H:"+uid);
				$.getScript( "https://brahma.analyticsvidhya.com/static/js/events.js" )
				  .done(function( script, textStatus ) {
				    console.log( "Brahma Loaded" );
				  })
				  .fail(function( jqxhr, settings, exception ) {
						console.log( "Brahma Failed" );
				});
								//$("#signup").hide();
				//$("#dwn-brchr").show();
	// End If user is logged in give option to bookmark article

}

function user_logged_out()
{
    //console.log("logged out");
		var page_url = $(location). attr("href");
    $("#login1").html("<a href='https://id.analyticsvidhya.com/auth/login/?next="+$(location).attr('href')+ "' target='_blank'> Login / Register</a>");
    $(".save-for-later").html("<a href='https://id.analyticsvidhya.com/auth/login/?next="+$(location).attr('href')+ "' target='_blank'>Login to Bookmark this article</a>");
		$("#fb-blue-href").attr("href","https://id.analyticsvidhya.com/auth/login/?next="+$(location).attr('href'));
		$("#fb-blue-href").attr("title","Login to Bookmark this article");
		$(".download-pdf").html("<a href='https://www.analyticsvidhya.com/back-channel/download-pdf.php?pid=36027' target='_blank'><img src='https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/download-post-pdf.png' alt='Click to Download PDF' title='Click to Download PDF of this Article'></a>");
		$("#top-av-users-show").hide();
		$("#top-av-users-signup").show();
		$("#is_logged_in_brchr").val("0");
		if ($(".coding-window-no-show").length == 0)
		{
			$(".coding-window").before("<a href='https://id.analyticsvidhya.com/auth/login/?next="+$(location).attr('href')+ "?&utm_source=coding-window-blog&source=coding-window-blog'><img src='https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2019/08/coding-window-noshow.jpg' class='coding-window-no-show'></a>");
		}
		$(".coding-window").hide();
		$(".fluffycourse").hide();
		$(".fluffycourse").after("<div 'fluffycourse-hide'><a href='https://id.analyticsvidhya.com/auth/login/?next="+$(location).attr('href')+ "?&utm_source=coding-window-blog&source=coding-window-blog'><img src='https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2019/08/test-learning.jpg' class='coding-window-no-show'></a></div>");
		// Popup Logged out
		$("#popup_logged_in").val("0");
		uid = "";
		//$("#signup").show();
		//$("#dwn-brchr").hide();
		$.getScript( "https://brahma.analyticsvidhya.com/static/js/events.js" )
			.done(function( script, textStatus ) {
				console.log( "Brahma Script Loaded" );
			})
			.fail(function( jqxhr, settings, exception ) {
				console.log( "Brahma Script Failed" );
		});
		}

function quizInit(uid)
{
			return;
	}

function initCodingWindow() {
	$('.coding-window iframe').each(function() {
		var src = $(this).data('src');
		if (!src) {
			return;
		}
		this.src = src;
	})
}

$(document).ready(function(){

        var img_path = "https://www.analyticsvidhya.com/wp-content/themes/Curated/";


mgr.getUser().then(function(user) {
    // If user was already logged in
    if (user!==null){
        mgr.querySessionStatus().then(function(res) {
        //console.log('Ohhh, its you again');
        av_is_signed_in = true;
        // Log the user in
        user_logged_in(user);

        }).catch(function(err){
            // If user logged out from OP
            av_is_signed_in = false;
            user_logged_out();
            mgr.removeUser();
        });

    } // End check if the user is not null
    else{
        mgr.signinRedirectCallback().then(function(user) {
            // If user just logged in and redirected to this page
            //console.log(user.profile.given_name, ' bro, how was login experience?');
            av_is_signed_in = true;
            // Call user_logged_in_functions
            user_logged_in(user);
        }).catch(function(err) {
            // If user is NOT logged in
            //console.log('Logged out');
            user_logged_out();
            mgr.signinSilent().then(function(user) {
                av_is_signed_in = true;
                //console.log("silent signed in", user);
                user_logged_in(user);
            }).catch(function(err) {
                av_is_signed_in = false;
                user_logged_out();
                //console.log("Silentsignin Err");
               // console.log(err);
            });

        });
    }
}).catch(function(err){
    //console.log('Couldnt get user');
    user_logged_out();
});


}); // document.ready


// Bookmark Article
$('.fb-blue').click(function(){
	if (av_is_signed_in)
		bookmark_post();
});

$('.save-for-later').on("click", "#bookmark-article",function(event){
	bookmark_post();
});

function bookmark_post()
{
	event.preventDefault();
	var bookmark_data = {resource_id:"36027", resource_type:"article",resource_permalink:"https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/",resource_title:"An Intuitive Understanding of  Word Embeddings: From Count Vectors to Word2Vec",resource_medium: "blog"};
 if (is_bookmarked==false) {
 $.ajax({
	type:"POST",
	dataType:"json",
	url:"https://id.analyticsvidhya.com/api/v1/resources/external_save",
	data:JSON.stringify(bookmark_data),
 contentType: 'application/json',
	success: function(data) {
			data =JSON.stringify(data);
			 $(".save-for-later").html('<a id="bookmark-article" href="#"><img src="'+img_path+'/custom-design/images/bookmarked.svg" width="18px" alt="Article Bookmarked" title="Article Bookmarked" data-toggle="tooltip"></a>');
			 $(".fb-blue").addClass("fb-active-blue");
			 $("#fb-blue-href").attr("title","Article Bookmarked");
			 is_bookmarked=true;
	},
		 error: function(data) {
		 data =  JSON.stringify(data);
		 //$(".save-for-later").html(data);
		 $(".save-for-later").html('Failed to Bookmark Article');
		 $(".fb-blue").removeClass("fb-active-blue");
		 $("#fb-blue-href").attr("title","Failed to Bookmark Article");
		}
 });
} // end if
else if (is_bookmarked==true){
 //console.log("Remove");
 $.ajax({
	type:"DELETE",
	dataType:"json",
	url:"https://id.analyticsvidhya.com/api/v1/resources/external_save",
	data:JSON.stringify(bookmark_data),
 contentType: 'application/json',
	success: function(data) {
			data =JSON.stringify(data);
			$(".save-for-later").html('<a id="bookmark-article" href="#"><img src="'+img_path+'/custom-design/images/do-bookmark.svg" width="18px" alt="Bookmark this story to read later" title="Bookmark this story to read later" data-toggle="tooltip"></a>');
			$(".fb-blue").removeClass("fb-active-blue");
			$("#fb-blue-href").attr("title","Bookmark this story to read later");
			 is_bookmarked=false;
	},
		 error: function(data) {
		 data =  JSON.stringify(data);
		 //$(".save-for-later").html(data);
		 $(".save-for-later").html('Failed to Remove Bookmark');
		 $("#fb-blue-href").attr("title","Failed to Remove Bookmark");

		}
 });
} //end elseif

}


</script>
<script>
jQuery(document).ready(function(){
	var articles1 = [21948,23238,34673,40972,48985,25318,57430,59396];
	var articles2 = [18835,18770,36077,22831,31141,36659,46495];
	var articles3 = [22185,15335,22387,62604,6377,21124,27576,20925]
	var articles4 = [21124,62205,43910,39175];
	var cur_article = 36027;

	//time series banner
	if (jQuery.inArray(cur_article, articles1) !='-1'){
		jQuery("#time-series-disp").html('<a href="https://courses.analyticsvidhya.com/courses/creating-time-series-forecast-using-python?utm_source=timesereisbanner&utm_medium=blog" target="_blank"><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2020/02/7-2.png"></a>');

		//Pandas for data analysis course
	}else if (jQuery.inArray(cur_article, articles3) !='-1'){
			jQuery("#time-series-disp").html('<a href="https://courses.analyticsvidhya.com/courses/pandas-for-data-analysis-in-python?utm_source=pandascoursebanner&utm_medium=blog" target="_blank"><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2020/03/3-1.png"></a>');
			//Regression analysis data
	}else if (jQuery.inArray(cur_article, articles2) !='-1'){
			jQuery("#time-series-disp").html('<a href="https://courses.analyticsvidhya.com/courses/Fundamentals-of-Regression-Analysis?utm_source=regressionbanner&utm_medium=blog" target="_blank"><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2020/03/2.jpg"></a>');
	}else if (jQuery.inArray(cur_article, articles4) !='-1'){
				jQuery("#time-series-disp").html('<a href="https://courses.analyticsvidhya.com/courses/introduction-to-analytics?utm_source=BusinessAnalyticsBanner&utm_medium=blog" target="_blank"><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2020/05/intro-to-bs.png"></a>');
			//default course banner
	}else{
		jQuery("#time-series-disp").html('<a href="https://courses.analyticsvidhya.com/courses/data-science-hacks-tips-and-tricks?utm_source=hacksandtipsbanner&utm_medium=blog" target="_blank"><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/free-ds.png"></a>');
	}
});

</script>

<script>
$(document).ready(function(){
	var article_url = 'https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/';
	$.ajax({
		method:'post',
		url:'https://www.analyticsvidhya.com/back-channel/get-brochure-message.php',
		data:{'article_url':article_url},
		success:function(response){
			var obj = JSON.parse(response);
			$('#dwn-post-id').val(obj.article_url);
			$('#for-download-pdf').val(obj.brochure_download);
			if(obj.brochure_download == 1){
				var message = "Start Your Business Analytics Journey";
				var sub_title = "Become A Certified Business Analytics Professional";
				var button_text = "Learn Business Analytics";
			}
			if(obj.brochure_download == 2){
				var message = "Start Your Machine Learning Journey";
				var sub_title = "Explore Applied Machine Learning Course";
				var button_text = "Learn Machine Learning";
			}
			if(obj.brochure_download == 3){
				var message = "Start Your NLP Journey";
				var sub_title = "Explore Natural Language Processing Course";
				var button_text = "Learn NLP";
			}
			if(obj.brochure_download == 4){
				var message = "Start Your Deep Learning Journey";
				var sub_title = "Explore Full Course on Deep Learning";
				var button_text = "Learn Deep Learning";
			}
			$('#brochure-sub-title').html(sub_title);
			$('#dwn-brochure-message').html(message);
			$('#dgd_scrollbox-65670-tab').html(button_text);
			$('#for-download-pdf').val(obj.message);
		},
		error:function(){
			// alert("Failed");
		}
	});
})
</script>

<script>
$(document).ready(function(){
$("#jobs-show").load("https://jobsnew.analyticsvidhya.com/blog/jobs");
});
</script>

<script>
$(document).ready(function(){
	$('#download_brochure_btn').on('click',function(){
		event.preventDefault();
		var article_url = $('#dwn-post-id').val();
		var name = $('#dwn-name').val();
		var phone = $('#dwn-phone').val();
		var email = $('#dwn-email').val();
		var bid = $('#for-download-pdf').val();
		if(name!=""){
			if(phone!=""){
				$.ajax({
					method: 'post',
					data:{'name':name,'phone':phone,'email':email,'article_url':article_url,'bid':bid},
					url:'https://www.analyticsvidhya.com/back-channel/download-brochure.php',
					success:function(response){
						if (response == 1)
							{
							$('#dwn-brochure-message').html('Thank you for your Interest');
							$('#dwn-brochure-frm').hide();
							// $('#dwn-error').hide();
							window.location = 'https://www.analyticsvidhya.com/back-channel/download-brochure-file.php?brochure_id='+bid+'&email='+email;
							}
						else 	$('#dwn-error').html(response);
						//$('#hide-download-brochure').html(response);
					},
					error:function(){
						$('#dwn-error').html('Some error occured');
						return false;
						// alert('Failed');
					}
				});
			}else{
				$('#dwn-error').html('Please enter phone');
				return false;
			}
		}else{
			$('#dwn-error').html('Please enter name');
			return false;
		}
	});
});
</script>

<script>
/* Use
			$(document).ready(function() {

		     $('#brochure-btn2').click(function(e) {
		        e.preventDefault();
		       	window.location.replace("https://id.analyticsvidhya.com/auth/login/?next="+window.location.href);
		     });

        $('#brochure-btn').click(function(e) {
*/
				/* Do not use
								if ($('#dwn_bcr_name').val() == ""){
									$("#error").empty();
									$('#error').addClass('error-show');
									$("#dwn_bcr_email").css("margin-top", "6%");
									$('#error').append('Please Enter Name');
									$('#dwn_bcr_name').focus();
									return false;
							}
							else*/
							/* Use
							 if ($('#dwn_bcr_email').val() == ""){
									$("#error1").empty();
									$("#dwn_bcr_email").css({ 'margin-top' : '', '6%' : '' });
									$('#error1').addClass('error-show');
									$('#error1').append('Please Enter Email Id');
									$("#dwn_bcr_phone").css("margin-top", "6%");
									$('#dwn_bcr_email').focus();
									return false;
							}
							/* Do not Use
							else if ($('#dwn_bcr_phone').val() == ""){
									$("#error2").empty();
									$("#dwn_bcr_phone").css({ 'margin-top' : '', '6%' : '' });
									$('#error2').addClass('error-show');
									$('#error2').append('Please Enter Phone Number');
									$('#dwn_bcr_phone').focus();
									return false;
							}*/
							/* Use
            e.preventDefault();
            $.ajax({
                type: "POST",
                url: 'https://www.analyticsvidhya.com/back-channel/download-brochure.php',
                data: $('#download-brochure').serialize(),
                beforeSend: function() {
                    $("#loading-image").show();

                },
                success: function(data){
                    $('#grdnt-clr').replaceWith('<div class="thnk-top">Thanks for showing your interest.</div>');
                    $('#frm-cntnt').replaceWith('<div class="thnk-cntn">' + data + '</div>');
                    // $('#frm-cntnt').html(data);
                    $("#loading-image").hide();
										if (av_is_signed_in == false)
										{
											try {
											window.location.replace("https://id.analyticsvidhya.com/auth/login/?next="+window.location.href);
											}
											catch(e) {
												window.location = "https://id.analyticsvidhya.com/auth/login/?next="+window.location.href;
											}
									  }
								}
            });
        });
    });*/
</script>
<script>
    function myFunction1() {
        document.getElementById("error").innerHTML = "";
    }
    function myFunction2() {
        var email = document.getElementById("dwn_bcr_email").value;
        var re = /^(([^<>()\[\]\\.,;:\s@"]+(\.[^<>()\[\]\\.,;:\s@"]+)*)|(".+"))@((\[[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\])|(([a-zA-Z\-0-9]+\.)+[a-zA-Z]{2,}))$/;
        if(email.match(re)){
            document.getElementById("error1").innerHTML = "";
            return true;
        }else{
            document.getElementById("error1").classList.add("error-show");
            document.getElementById("error1").innerHTML = "Please enter valid email id";
            return false;
        }
    }
    function myFunction3() {
        document.getElementById("error2").innerHTML = "";
    }
</script>


<script type="text/javascript" id="contact-form-7-js-extra">
/* <![CDATA[ */
var wpcf7 = {"apiSettings":{"root":"https:\/\/www.analyticsvidhya.com\/wp-json\/contact-form-7\/v1","namespace":"contact-form-7\/v1"},"cached":"1"};
/* ]]> */
</script>
<script type="text/javascript" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/scripts.js" defer="defer" id="contact-form-7-js"></script>
<script type="text/javascript" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/main.js" id="maha-main-js"></script><style type="text/css">.marquee{white-space:nowrap;overflow:hidden;visibility:hidden;}#marq_kill_marg_bord{border:none!important;margin:0!important;}</style>
<script type="text/javascript" id="maha-basix-js-extra">
/* <![CDATA[ */
var MahaAjax = {"ajaxurl":"https:\/\/www.analyticsvidhya.com\/wp-admin\/admin-ajax.php"};
/* ]]> */
</script>
<script type="text/javascript" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/basix.js" defer="defer" id="maha-basix-js"></script>
<script type="text/javascript" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/add_to_cart.js" defer="defer" id="add_to_cart-js"></script>
<script type="text/javascript" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/comment-reply.min.js" defer="defer" id="comment-reply-js"></script>
<!--[if IE 9]>
<script type='text/javascript' src='//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js?ver=1' defer='defer' id='maha-html5-ie9-js'></script>
<![endif]-->
<!--[if IE 9]>
<script type='text/javascript' src='//cdnjs.cloudflare.com/ajax/libs/respond.js/1.3.0/respond.min.js?ver=1' defer='defer' id='maha-respond-ie9-js'></script>
<![endif]-->
<script type="text/javascript" id="q2w3_fixed_widget-js-extra">
/* <![CDATA[ */
var q2w3_sidebar_options = [{"sidebar":"blog-sidebar","margin_top":55,"margin_bottom":800,"stop_id":"","screen_max_width":0,"screen_max_height":0,"width_inherit":false,"refresh_interval":1500,"window_load_hook":false,"disable_mo_api":false,"widgets":["text-42","text-45"]}];
/* ]]> */
</script>
<script type="text/javascript" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/q2w3-fixed-widget.min.js" defer="defer" id="q2w3_fixed_widget-js"></script>
<script type="text/javascript" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/effect.min.js" defer="defer" id="jquery-effects-core-js"></script>
<script type="text/javascript" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/effect-blind.min.js" defer="defer" id="jquery-effects-blind-js"></script>
<script type="text/javascript" id="wstbLayout-js-extra">
/* <![CDATA[ */
var stbUserOptions = {"mode":"css","cssOptions":{"roundedCorners":true,"mbottom":10,"imgHide":"https:\/\/www.analyticsvidhya.com\/blog\/wp-content\/plugins\/wp-special-textboxes\/images\/minus.png","imgShow":"https:\/\/www.analyticsvidhya.com\/blog\/wp-content\/plugins\/wp-special-textboxes\/images\/plus.png","strHide":"Hide","strShow":"Show"}};
/* ]]> */
</script>
<script type="text/javascript" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/wstb.min.js" defer="defer" id="wstbLayout-js"></script>
<script type="text/javascript" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/wp-embed.min.js" defer="defer" id="wp-embed-js"></script>

<div class="dgd_overlay" style="height: 26059px; opacity: 0.7; background-color: rgb(0, 0, 0); display: none;"></div>
<div class="dgd_stb_box none" id="dgd_scrollbox-67322" style="padding: 0px; height: 500px; width: 800px; top: 597px; left: 240px; display: none;"><a class="dgd_stb_box_close dgd_stb_box_x" href="javascript:void(0);"> </a><p><a href="https://datahack.analyticsvidhya.com/contest/hacklive-2-guided-community-hackathon/?utm_source=Blog&amp;utm_medium=popup&amp;utm_campaign=hacklive" target="_blank" rel="noopener noreferrer"><img src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/HackLive2.webp"></a></p>
<link rel="stylesheet" type="text/css" href="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/style(1).css"><ul class="stb_social"></ul></div>

<script type="text/javascript" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/e-202040.js" async="async" defer="defer"></script>
<script type="text/javascript">
	_stq = window._stq || [];
	_stq.push([ 'view', {v:'ext',j:'1:8.8.1',blog:'180026056',post:'36027',tz:'5.5',srv:'www.analyticsvidhya.com'} ]);
	_stq.push([ 'clickTrackerInit', '180026056', '36027' ]);
</script>
<link rel="stylesheet" href="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/strip.css">

<script type="text/javascript" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/getFlashDetails.js" defer=""></script>

<script src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/moment-with-locales.min.js"></script>



<div class="dgd_overlay" style="height: 26059px; opacity: 0.7; background-color: rgb(0, 0, 0); display: none;"></div>
<div class="dgd_stb_box none" id="dgd_scrollbox-67322"><a class="dgd_stb_box_close dgd_stb_box_x" href="javascript:void(0);"> </a><p><a href="https://datahack.analyticsvidhya.com/contest/hacklive-2-guided-community-hackathon/?utm_source=Blog&amp;utm_medium=popup&amp;utm_campaign=hacklive" target="_blank" rel="noopener noreferrer"><img src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/HackLive2.webp"></a></p>
</div>

<script type="text/javascript" src="./Understanding Word Embeddings_ From Word2Vec to Count Vectors_files/e-202040.js" async="async" defer="defer"></script>
<script type="text/javascript">
	_stq = window._stq || [];
	_stq.push([ 'view', {v:'ext',j:'1:8.8.1',blog:'180026056',post:'36027',tz:'5.5',srv:'www.analyticsvidhya.com'} ]);
	_stq.push([ 'clickTrackerInit', '180026056', '36027' ]);
</script>


</body></html>